# AUTOGENERATED! DO NOT EDIT! File to edit: train.ipynb.

# %% auto 0
__all__ = ['seed', 'dtype', 'device', 'debug', 'ddp', 'model_config', 'data_config', 'train_config', 'transformer',
           'text_encoder', 'tokenizer', 'dcae', 'ds', 'latent_shape', 'collate_fn', 'dataloader_train',
           'dataloader_eval', 'optimizer', 'wandb_run', 'steps_epoch', 'step', 'last_step_time', 'load_models',
           'load_data', 'collate_', 'get_dataloader', 'get_timesteps', 'generate', 'add_random_noise', 'eval_loss']

# %% train.ipynb 3
import torch, torch.nn.functional as F, random, wandb, time
import torchvision.transforms as T
from torchvision import transforms
from diffusers import AutoencoderDC, SanaTransformer2DModel
from diffusers.schedulers import FlowMatchEulerDiscreteScheduler
from transformers import AutoModel, AutoTokenizer, set_seed
from datasets import load_dataset, Dataset, DatasetDict
from tqdm import tqdm
from torch.utils.data import DataLoader, RandomSampler
from functools import partial
from types import SimpleNamespace
import torch.distributed as dist
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel

from utils import pil_add_text, latent_to_PIL, make_grid, fmnist_labels, encode_prompt, dcae_scalingf, pil_clipscore, cifar10_labels, free_memory, mnist_labels

seed = 42
set_seed(seed)


# %% train.ipynb 4
def load_models(text_encoder, transformer_config, ae, dtype, device):
    transformer = SanaTransformer2DModel.from_config(transformer_config).to(device).to(dtype)
    te = AutoModel.from_pretrained(text_encoder, torch_dtype=dtype).to(device)
    tok = AutoTokenizer.from_pretrained(text_encoder, torch_dtype=dtype)
    dcae = AutoencoderDC.from_pretrained(ae, subfolder="vae", torch_dtype=dtype).to(device)

    if debug:
        print(f"Transformer parameters: {sum(p.numel() for p in transformer.parameters()) / 1e6:.2f}M")
        print(f"DCAE parameters: {sum(p.numel() for p in dcae.parameters()) / 1e6:.2f}M")

    return transformer, te, tok, dcae

def load_data(repo_name, split_train="train", split_test="test", col_latent = "latent", col_label = "label"):
    print(f"loading dataset {repo_name}")
    
    ds = load_dataset(repo_name)
    for split in ds: 
        if not split in [split_train, split_test]: 
            del ds[split]
    latent_shape = torch.Tensor(ds[split_train][0][col_latent]).shape
    features = ds[split_train].features

    assert len(latent_shape)==4
    assert col_latent in features and col_label in features
    assert ds[split_train].features==ds[split_test].features

    if debug:
        for i, split in enumerate([split_train, split_test]): 
            print(f" split #{i} {split}: {len(ds[split])} samples, features: {[k for k in ds[split].features]}")
        print(f" latent shape {latent_shape}")

    return ds, latent_shape
    
def collate_(items, labels_encoded, col_latent = "latent", col_label = "label"):
    assert col_latent in items[0] and col_label in items[0]
    labels = [i[col_label] for i in items]
    latents = torch.cat([torch.Tensor(i[col_latent]) for i in items]).to(dtype).to(device)
    prompts_encoded = torch.cat([labels_encoded[label][0] for label in labels])
    prompts_atnmask = torch.cat([labels_encoded[label][1] for label in labels])

    return labels, latents, prompts_encoded, prompts_atnmask

def get_dataloader(dataset, batch_size, collate_fn):
    if ddp:
        sampler = DistributedSampler(dataset, shuffle = True, seed = seed)
        # dataloader = DataLoader(
        #     dataset=dataset, 
        #     batch_size=batch_size, 
        #     shuffle=False, 
        #     sampler=sampler,
        #     collate_fn=collate_fn,
        #     # pin_memory = True,
        # )
    else:
        sampler = RandomSampler(dataset, generator = torch.manual_seed(seed))
        
    dataloader = DataLoader(
        dataset, 
        batch_size = batch_size, 
        sampler = sampler,
        collate_fn = collate_fn
    )
    
    if debug:
        print("dataset size", len(dataset))
        for i, col in enumerate(next(iter(dataloader))):
            coltype = type(col)
            collength = len(col) if coltype==list else col.shape
            print(f" col {i} {coltype.__name__} {collength}")
    return dataloader
    
def get_timesteps(num_steps):
    dt = 1.0 / num_steps
    timesteps = [int(i/num_steps*1000) for i in range(num_steps, 0, -1)]
    return dt, timesteps

def generate(prompt, tokenizer, text_encoder, latent_dim=None, num_steps=100, latent_seed=None):
    assert latent_dim is not None
    dt, timesteps = get_timesteps(num_steps)
    prompt_encoded, prompt_atnmask = encode_prompt(str(prompt), tokenizer, text_encoder)
    latent = torch.randn(latent_dim, generator=torch.manual_seed(latent_seed) if latent_seed else None).to(dtype).to(device)
    for t in timesteps:
        t = torch.Tensor([t]).to(dtype).to(device)
        with torch.no_grad():
            noise_pred = transformer(latent, encoder_hidden_states=prompt_encoded, timestep=t, encoder_attention_mask=prompt_atnmask, return_dict=False)[0]
        latent = latent - dt * noise_pred

    return latent_to_PIL(latent / dcae_scalingf, dcae)

def add_random_noise(latents, timesteps=1000):
    noise_dist = train_config.timestep_sampling
    bs = latents.size(0)
    noise = torch.randn_like(latents)
    if noise_dist=="uniform":
        t = torch.randint(1, timesteps + 1, (bs,)).to(device)
        # t = torch.Tensor([900]*bs).to(torch.int64).to(device)
    elif noise_dist=="beta":
        beta = torch.distributions.beta.Beta(torch.tensor(1), torch.tensor(2.5))
        t = (beta.sample([bs])*timesteps).to(torch.int64).to(device)
    tperc = t.view([latents.size(0), *([1] * len(latents.shape[1:]))])/timesteps
    latents_noisy = (1 - tperc) * latents + tperc * noise # (1-noise_level) * latent + noise_level * noise

    return latents_noisy, noise, t

def eval_loss(dataloader_eval, timesteps=1000, testing=False):
    losses = []

    for batch_num, (labels, latents, prompts_encoded, prompts_atnmask) in tqdm(enumerate(dataloader_eval), "eval_loss"):
        latents = latents * dcae_scalingf
        latents_noisy, noise, t = add_random_noise(latents, timesteps)
        with torch.no_grad():
            noise_pred = transformer(latents_noisy.to(dtype), prompts_encoded, t, prompts_atnmask).sample
    
        loss = F.mse_loss(noise_pred, noise - latents)
        losses.append(loss.item())  
        if testing: break
    return sum(losses)/len(losses)

# %% train.ipynb 5
dtype = torch.bfloat16
device = "cuda" if torch.cuda.is_available() else "mps" if torch.mps.is_available() else "cpu"
debug = True
ddp = True

model_config = SimpleNamespace(
    text_encoder = "answerdotai/ModernBERT-base",
    transformer_config = "transformer_Sana-DiT-S-MBERT.json",
    ae = "Efficient-Large-Model/Sana_600M_1024px_diffusers",
)

data_config = SimpleNamespace(
    dataset = "g-ronimo/FMNIST-latents-64_dc-ae-f32c32-sana-1.0",
    labels_dict = fmnist_labels,
    col_label = "label",
    col_latent = "latent",
    split_train = "train",
    split_eval = "test",
)

train_config = SimpleNamespace(
    lr = 5e-4,
    bs = 1024,
    epochs = 20,
    steps_log = 10,
    steps_eval = 100,
    eval_prompts = [data_config.labels_dict[k] for k in data_config.labels_dict],
    eval_seeds = [6945, 4009, 1479, 8141, 3441], # seeds for latent generation
    eval_timesteps = 100, # number of timesteps for generating eval images
    timesteps_training = 1000,
    timestep_sampling = "beta",  # beta uniform
    log_wandb = True,
    wandb_project = "Hana",
    wandb_run = "Sana-DiT-S-{size:.2f}M_{ds}_LR-{lr}_BS-{bs}_{ts_sampling}-TS-{ts}_{device}",
)

# %% train.ipynb 7
if ddp:
    dist.init_process_group(backend='nccl')
    is_master = dist.get_rank() == 0  
    world_size = dist.get_world_size()
    local_rank = dist.get_rank()
    torch.cuda.set_device(local_rank)
    bs_new = round(train_config.bs / world_size / 8) * 8
    if is_master:
        print(f"DDP: global batch size {train_config.bs} with world of {world_size} => setting per_device_bs to {bs_new}")
    train_config.bs = bs_new
    debug = debug and is_master
else:
    is_master = True
    world_size = 1
    local_rank = 0

transformer, text_encoder, tokenizer, dcae = load_models(
    text_encoder = model_config.text_encoder,
    transformer_config = model_config.transformer_config,
    ae = model_config.ae,
    dtype = dtype,
    device = device
)
if ddp:
	transformer = DistributedDataParallel(transformer, device_ids=[local_rank])
    
ds, latent_shape = load_data(data_config.dataset)

collate_fn = partial(
    collate_, 
    labels_encoded = {
        k: encode_prompt(data_config.labels_dict[k], tokenizer, text_encoder) 
        for k in data_config.labels_dict
    }
)
dataloader_train = get_dataloader(ds[data_config.split_train], train_config.bs, collate_fn)
dataloader_eval =  get_dataloader(ds[data_config.split_eval], train_config.bs, collate_fn)

optimizer = torch.optim.AdamW(transformer.parameters(), lr=train_config.lr)

wandb_run = train_config.wandb_run.format(
    size=sum(p.numel() for p in transformer.parameters())/1e6, 
    lr=train_config.lr, 
    bs=train_config.bs, 
    ts=train_config.timesteps_training,
    ts_sampling=train_config.timestep_sampling.upper(),
    device=device,
    ds=data_config.dataset.split("/")[1].split("-")[0]
)

steps_epoch = len(dataloader_train)
if is_master: 
    print(f"steps per epoch: {steps_epoch}")

# %% train.ipynb 11
free_memory()

if is_master and train_config.log_wandb: 
    if wandb.run is not None: wandb.finish()
    wandb.init(
        project=train_config.wandb_project, 
        name=wandb_run
    ).log_code(".", include_fn=lambda path: path.endswith(".py") or path.endswith(".ipynb") or path.endswith(".json"))

step = 0
last_step_time = time.time()
free_memory()

for _ in range(train_config.epochs):
    for labels, latents, prompts_encoded, prompts_atnmask in dataloader_train:
        latents = latents * dcae_scalingf
        latents_noisy, noise, t = add_random_noise(latents)
        noise_pred = transformer(latents_noisy.to(dtype), prompts_encoded, t, prompts_atnmask).sample
        loss = F.mse_loss(noise_pred, noise - latents)
        
        loss.backward()
        grad_norm = torch.nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)
        optimizer.step()
        optimizer.zero_grad()    
        
        if is_master and step>0 and step % train_config.steps_log == 0:
            loss_train = loss.item()
            step_time = (time.time() - last_step_time) / train_config.steps_log * 1000
            sample_count = step * train_config.bs * world_size
            sample_tp = train_config.bs * train_config.steps_log * world_size / (time.time() - last_step_time)
            print(f"step {step}, epoch: {step / steps_epoch:.2f}, train loss: {loss_train:.4f}, grad_norm: {grad_norm:.2f}, {step_time:.2f}ms/step, {sample_tp:.2f}samples/sec")
            if train_config.log_wandb: 
                wandb.log({"loss_train": loss_train, "grad_norm": grad_norm, "step_time": step_time, "step": step, "sample_tp": sample_tp, "sample_count": sample_count, "epoch": step / steps_epoch})
            last_step_time = time.time()

        if is_master and step>0 and step % train_config.steps_eval == 0:
            transformer.eval()
            loss_eval = eval_loss(dataloader_eval)

            # try different seeds for generating eval images
            images_eval = [
                generate(p, tokenizer, text_encoder, num_steps=train_config.eval_timesteps, latent_dim=latent_shape, latent_seed=seed) 
                for seed in tqdm(train_config.eval_seeds, "eval_images")
                for p in train_config.eval_prompts
            ]
            clipscore = pil_clipscore(images_eval, train_config.eval_prompts * len(train_config.eval_seeds))
            # add labels before logging the images
            images_eval = make_grid([
                pil_add_text(img, train_config.eval_prompts[i % len(train_config.eval_prompts)]) 
                for i, img in enumerate(images_eval)
            ], rows=len(train_config.eval_seeds), cols=len(train_config.eval_prompts))
            print(f"step {step}, eval loss: {loss_eval:.4f}, clipscore: {clipscore:.2f}")
            if train_config.log_wandb: 
                wandb.log({"loss_eval": loss_eval, "clipscore": clipscore, "images_eval": wandb.Image(images_eval), "step": step, "sample_count": step * train_config.bs, "epoch": step / steps_epoch})
            transformer.train()        

        step += 1

if ddp:
    dist.destroy_process_group()
