# AUTOGENERATED! DO NOT EDIT! File to edit: train.ipynb.

# %% auto 0
__all__ = ['seed', 'data_config', 'train_config', 'dtype', 'device', 'debug', 'log_wandb', 'wandb_project', 'transformer',
           'text_encoder', 'tokenizer', 'dcae', 'ds', 'latent_shape', 'collate_fn', 'dataloader_train',
           'dataloader_eval', 'optimizer', 'wandb_run', 'steps_epoch', 'step', 'last_step_time', 'load_models',
           'load_data', 'collate_', 'get_dataloader', 'get_timesteps', 'generate', 'add_random_noise', 'eval_loss']

# %% train.ipynb 3
import torch, torch.nn.functional as F, random, wandb, time
import torchvision.transforms as T
from torchvision import transforms
from diffusers import AutoencoderDC, SanaTransformer2DModel
from diffusers.schedulers import FlowMatchEulerDiscreteScheduler
from transformers import AutoModel, AutoTokenizer, set_seed
from datasets import load_dataset, Dataset, DatasetDict
from tqdm import tqdm
from torch.utils.data import DataLoader
from functools import partial
from types import SimpleNamespace

from utils import pil_add_text, latent_to_PIL, make_grid, fmnist_labels, encode_prompt, dcae_scalingf, pil_clipscore, cifar10_labels, free_memory, mnist_labels

seed = 42
set_seed(seed)


# %% train.ipynb 4
def load_models(text_encoder, transformer_config, ae, dtype, device):
    transformer = SanaTransformer2DModel.from_config(transformer_config).to(device).to(dtype)
    te = AutoModel.from_pretrained(text_encoder, torch_dtype=dtype).to(device)
    tok = AutoTokenizer.from_pretrained(text_encoder, torch_dtype=dtype)
    dcae = AutoencoderDC.from_pretrained(ae, subfolder="vae", torch_dtype=dtype).to(device)

    if debug:
        print(f"Transformer parameters: {sum(p.numel() for p in transformer.parameters()) / 1e6:.2f}M")
        print(f"DCAE parameters: {sum(p.numel() for p in dcae.parameters()) / 1e6:.2f}M")

    return transformer, te, tok, dcae

def load_data(repo_name, split_train="train", split_test="test", col_latent = "latent", col_label = "label"):
    print(f"loading dataset {repo_name}")
    
    ds = load_dataset(repo_name)
    for split in ds: 
        if not split in [split_train, split_test]: 
            del ds[split]
    latent_shape = torch.Tensor(ds[split_train][0][col_latent]).shape
    features = ds[split_train].features

    assert len(latent_shape)==4
    assert col_latent in features and col_label in features
    assert ds[split_train].features==ds[split_test].features

    if debug:
        for i, split in enumerate([split_train, split_test]): 
            print(f" split #{i} {split}: {len(ds[split])} samples, features: {[k for k in ds[split].features]}")
        print(f" latent shape {latent_shape}")

    return ds, latent_shape
    
def collate_(items, labels_encoded, col_latent = "latent", col_label = "label"):
    assert col_latent in items[0] and col_label in items[0]
    labels = [i[col_label] for i in items]
    latents = torch.cat([torch.Tensor(i[col_latent]) for i in items]).to(dtype).to(device)
    prompts_encoded = torch.cat([labels_encoded[label][0] for label in labels])
    prompts_atnmask = torch.cat([labels_encoded[label][1] for label in labels])

    return labels, latents, prompts_encoded, prompts_atnmask

def get_dataloader(dataset, batch_size, collate_fn):
    dataloader = DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        generator = torch.manual_seed(seed), 
        collate_fn = collate_fn
    )
    if debug:
        print("dataset size", len(dataset))
        for i, col in enumerate(next(iter(dataloader))):
            coltype = type(col)
            collength = len(col) if coltype==list else col.shape
            print(f" col {i} {coltype.__name__} {collength}")
    return dataloader
    
def get_timesteps(num_steps):
    dt = 1.0 / num_steps
    timesteps = [int(i/num_steps*1000) for i in range(num_steps, 0, -1)]
    return dt, timesteps

def generate(prompt, tokenizer, text_encoder, latent_dim=None, num_steps=100, latent_seed=None):
    assert latent_dim is not None
    dt, timesteps = get_timesteps(num_steps)
    prompt_encoded, prompt_atnmask = encode_prompt(str(prompt), tokenizer, text_encoder)
    latent = torch.randn(latent_dim, generator=torch.manual_seed(latent_seed) if latent_seed else None).to(dtype).to(device)
    for t in timesteps:
        t = torch.Tensor([t]).to(dtype).to(device)
        with torch.no_grad():
            noise_pred = transformer(latent, encoder_hidden_states=prompt_encoded, timestep=t, encoder_attention_mask=prompt_atnmask, return_dict=False)[0]
        latent = latent - dt * noise_pred

    return latent_to_PIL(latent / dcae_scalingf, dcae)

def add_random_noise(latents, timesteps=1000):
    noise_dist = train_config.timestep_sampling
    bs = latents.size(0)
    noise = torch.randn_like(latents)
    if noise_dist=="uniform":
        t = torch.randint(1, timesteps + 1, (bs,)).to(device)
        # t = torch.Tensor([900]*bs).to(torch.int64).to(device)
    elif noise_dist=="beta":
        beta = torch.distributions.beta.Beta(torch.tensor(1), torch.tensor(2.5))
        t = (beta.sample([bs])*timesteps).to(torch.int64).to(device)
    tperc = t.view([latents.size(0), *([1] * len(latents.shape[1:]))])/timesteps
    latents_noisy = (1 - tperc) * latents + tperc * noise # (1-noise_level) * latent + noise_level * noise

    return latents_noisy, noise, t

def eval_loss(dataloader_eval, timesteps=1000, testing=False):
    losses = []

    for batch_num, (labels, latents, prompts_encoded, prompts_atnmask) in tqdm(enumerate(dataloader_eval), "eval_loss"):
        latents = latents * dcae_scalingf
        latents_noisy, noise, t = add_random_noise(latents, timesteps)
        with torch.no_grad():
            noise_pred = transformer(latents_noisy.to(dtype), prompts_encoded, t, prompts_atnmask).sample
    
        loss = F.mse_loss(noise_pred, noise - latents)
        losses.append(loss.item())  
        if testing: break
    return sum(losses)/len(losses)

# %% train.ipynb 5
data_config = SimpleNamespace(
    dataset = "g-ronimo/FMNIST-latents-64_dc-ae-f32c32-sana-1.0",
    labels_dict = fmnist_labels,
    col_label = "label",
    col_latent = "latent",
    split_train = "train",
    split_eval = "test",
)

train_config = SimpleNamespace(
    lr = 5e-4,
    bs = 1024,
    epochs = 100,
    steps_log = 10,
    steps_eval = 100,
    eval_prompts = [data_config.labels_dict[k] for k in data_config.labels_dict],
    eval_seeds = [6945, 4009, 1479, 8141, 3441], # seeds for latent generation
    eval_timesteps = 100, # number of timesteps for generating eval images
    timesteps_training = 1000,
    timestep_sampling = "beta",  # beta uniform
)

# %% train.ipynb 6
free_memory()

dtype = torch.bfloat16
device = "cuda" if torch.cuda.is_available() else "mps" if torch.mps.is_available() else "cpu"
debug = True

log_wandb = True
wandb_project = "Hana"

transformer, text_encoder, tokenizer, dcae = load_models(
    text_encoder = "answerdotai/ModernBERT-base",
    transformer_config = "transformer_Sana-DiT-S-MBERT.json",
    ae = "Efficient-Large-Model/Sana_600M_1024px_diffusers",
    dtype = dtype,
    device = device
)

ds, latent_shape = load_data(data_config.dataset)

collate_fn = partial(
    collate_, 
    labels_encoded = {
        k: encode_prompt(data_config.labels_dict[k], tokenizer, text_encoder) 
        for k in data_config.labels_dict
    }
)
dataloader_train = get_dataloader(ds[data_config.split_train], train_config.bs, collate_fn)
dataloader_eval =  get_dataloader(ds[data_config.split_eval], train_config.bs, collate_fn)
# eval_prompts = [labels[k] for k in labels]

optimizer = torch.optim.AdamW(transformer.parameters(), lr=train_config.lr)

wandb_run = "Sana-DiT-S-{size:.2f}M_{ds}_LR-{lr}_BS-{bs}_{ts_sampling}-TS-{ts}_{device}".format(
    size=sum(p.numel() for p in transformer.parameters())/1e6, 
    lr=train_config.lr, 
    bs=train_config.bs, 
    ts=train_config.timesteps_training,
    ts_sampling=train_config.timestep_sampling.upper(),
    device=device,
    ds=data_config.dataset.split("/")[1].split("-")[0]
)

steps_epoch = len(dataloader_train)
print(f"steps per epoch: {steps_epoch}")

# %% train.ipynb 10
if log_wandb: 
    if wandb.run is not None: wandb.finish()
    wandb.init(project=wandb_project, name=wandb_run).log_code(".", include_fn=lambda path: path.endswith(".py") or path.endswith(".ipynb") or path.endswith(".json"))

step = 0
last_step_time = time.time()
free_memory()

for _ in range(train_config.epochs):
    for labels, latents, prompts_encoded, prompts_atnmask in dataloader_train:
        latents = latents * dcae_scalingf
        latents_noisy, noise, t = add_random_noise(latents)
        noise_pred = transformer(latents_noisy.to(dtype), prompts_encoded, t, prompts_atnmask).sample
        loss = F.mse_loss(noise_pred, noise - latents)
        
        loss.backward()
        grad_norm = torch.nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)
        optimizer.step()
        optimizer.zero_grad()    
        
        if step>0 and step % train_config.steps_log == 0:
            loss_train = loss.item()
            step_time = (time.time() - last_step_time) / train_config.steps_log * 1000
            sample_tp = train_config.bs * train_config.steps_log / (time.time() - last_step_time)
            print(f"step {step}, epoch: {step / steps_epoch:.4f}, train loss: {loss_train:.4f}, grad_norm: {grad_norm:.2f}, {step_time:.2f}ms/step, {sample_tp:.2f}samples/sec")
            if log_wandb: wandb.log({"loss_train": loss_train, "grad_norm": grad_norm, "step_time": step_time, "step": step, "sample_tp": sample_tp, "sample_count": step * train_config.bs, "epoch": step / steps_epoch})
            last_step_time = time.time()

        if step % train_config.steps_eval == 0:
            transformer.eval()
            loss_eval = eval_loss(dataloader_eval)

            # try different seeds for generating eval images
            images_eval = [
                generate(p, tokenizer, text_encoder, num_steps=train_config.eval_timesteps, latent_dim=latent_shape, latent_seed=seed) 
                for seed in tqdm(train_config.eval_seeds, "eval_images")
                for p in train_config.eval_prompts
            ]
            clipscore = pil_clipscore(images_eval, train_config.eval_prompts * len(train_config.eval_seeds))
            # add labels before logging the images
            images_eval = make_grid([
                pil_add_text(img, train_config.eval_prompts[i % len(train_config.eval_prompts)]) 
                for i, img in enumerate(images_eval)
            ], rows=len(train_config.eval_seeds), cols=len(train_config.eval_prompts))
            print(f"step {step}, eval loss: {loss_eval:.4f}, clipscore: {clipscore:.2f}")
            if log_wandb: wandb.log({"loss_eval": loss_eval, "clipscore": clipscore, "images_eval": wandb.Image(images_eval), "step": step, "sample_count": step * train_config.bs, "epoch": step / steps_epoch})
            else: display(make_grid(images_eval, 2, 5))
            transformer.train()        

        step += 1

# %% train.ipynb 12
# # THE GALLERY!
# gallery_rows = []
# num_samples = 10
# for prompt in tqdm(eval_prompts, prompt):
#     row = make_grid([
#         generate(prompt, tokenizer, text_encoder, num_steps=100, latent_dim=latent_shape) 
#         for i in range(num_samples)
#     ], 1, num_samples)
#     row = pil_add_text(row, prompt, position=(1,1))
#     gallery_rows.append(row)
# make_grid(gallery_rows, 10, 1)
