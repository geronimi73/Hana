{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5722e1-0cc5-40b8-939a-986b390ad9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers accelerate datasets diffusers Pillow==9.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2e3054-0548-4534-a293-9f395c4d8998",
   "metadata": {},
   "source": [
    "# Imagenet-1k-recaptioned with AR\n",
    "* center cropping breaks a lot of images\n",
    "* instead, define aspect ratios, including portrait and lanscape\n",
    "* resize image to closest AR (mult. of 32px)\n",
    "* aspect crop\n",
    "* store in HF dataset, for each image include AR info\n",
    "* remove augmentations again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e0add2b-b9a5-4519-ae67-829f4a3c3654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import os\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from diffusers import AutoencoderDC\n",
    "# from torch.utils.data import DataLoader\n",
    "from utils import make_grid, PIL_to_latent, latent_to_PIL, dcae_scalingf\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils_preprocess import resize, pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff5c4b-cb6c-45bd-a435-88ba5477b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from local_secrets import hf_token\n",
    "# from huggingface_hub import login\n",
    "# login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ad5371-db09-42a4-aaf1-f396e4080292",
   "metadata": {},
   "source": [
    "# Load IN1k recaptions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17513fca-70d2-4a7a-a566-33f50d0831eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"visual-layer/imagenet-1k-vl-enriched\", cache_dir=\"~/ssd-2TB/hf_cache\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c199a4b6-e553-4964-bd9e-b40f0b73b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"splits\", ds.keys())\n",
    "print(\"features\", ds[\"train\"].features.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f541ed3-7577-4411-aaf9-903c3105b861",
   "metadata": {},
   "source": [
    "## Inspect augmentation before actually processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526836fb-ec1c-4ea4-97b4-c116499829e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Test run\n",
    "resizeTo = 256\n",
    "split = \"train\"\n",
    "\n",
    "ASPECT_RATIOS = [\n",
    "    (\"AR_1_to_1\", 1),\n",
    "    (\"AR_4_to_3\", 4/3),\n",
    "    (\"AR_3_to_4\", 3/4),\n",
    "]\n",
    "\n",
    "for i in [random.randint(0, len(ds[split])) for _ in range(10)]:\n",
    "    img=ds[split][i][\"image\"]\n",
    "    label=ds[split][i][\"caption_enriched\"]\n",
    "\n",
    "    print(\"image dimension\", img.size)\n",
    "    display(img)\n",
    "\n",
    "    images = []\n",
    "    images.append( pad(img).resize((256,256)) )\n",
    "    images.append( pad(resize(img, resizeTo=resizeTo, ARs=ASPECT_RATIOS, debug=True)[1]).resize((256,256)) )\n",
    "\n",
    "    print(label)\n",
    "    display(make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21db05f1-a7ad-4255-b2fe-3b9750e1be8d",
   "metadata": {},
   "source": [
    "# Load DCAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf9559-35ee-4751-aeca-d418a5bf21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Efficient-Large-Model/Sana_600M_1024px_diffusers\"\n",
    "dtype = torch.bfloat16\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "\n",
    "dcae = AutoencoderDC.from_pretrained(model, subfolder=\"vae\", torch_dtype=dtype).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8442c-3845-4468-b5a5-47274afeb378",
   "metadata": {},
   "source": [
    "# Batch augment and create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9fd5d96-f629-44eb-a90a-4d55ac84ce7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non DDP run\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1281167, 640584, 1281168)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    print(\"DDP run\")\n",
    "    world_size = dist.get_world_size()\n",
    "    rank = dist.get_rank()\n",
    "    pass\n",
    "else:\n",
    "    print(\"Non DDP run\")\n",
    "    rank = 0\n",
    "    world_size = 2\n",
    "\n",
    "split=\"train\"\n",
    "# ddp\n",
    "# ds[\"train\"]\n",
    "indices = list(range(len(ds[split])))\n",
    "indices_rank = indices[rank : len(ds[split]) : world_size]\n",
    "len(indices), len(indices_rank), len(indices_rank) * world_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a22f44f-f68a-4997-b5b8-60bcdae56b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank 0 [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
      "rank 1 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(20))\n",
    "world_size=2\n",
    "for rank in range(world_size):\n",
    "    indices_rank = indices[rank : len(ds[split]) : world_size]\n",
    "    print(\"rank\",rank,indices_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad78e46f-7b98-436b-88a0-7c3feeef344c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4003291929.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[31], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    [0:10:2]\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[0:10:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a1f859-dd1a-49d4-a011-3d18829895a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = True\n",
    "resizeTo = 256\n",
    "ASPECT_RATIOS = [\n",
    "    (\"AR_1_to_1\", 1),\n",
    "    (\"AR_4_to_3\", 4/3),\n",
    "    (\"AR_3_to_4\", 3/4),\n",
    "]\n",
    "hf_dataset = \"g-ronimo/IN1k256-AR-buckets-latents_dc-ae-f32c32-sana-1.0\"\n",
    "dcae_batch_size = 32\n",
    "upload_every = 100_000\n",
    "splits=[\"train\", \"validation\"]\n",
    "col_img=\"image\"\n",
    "col_label=\"caption_enriched\"\n",
    "\n",
    "def process_dcae_batch(batch):\n",
    "    labels = batch[\"labels\"]\n",
    "    images = batch[\"images\"]\n",
    "    latents = PIL_to_latent(images, dcae).cpu()   \n",
    "    \n",
    "    return [\n",
    "        dict(label=label, latent = latents[None,i])\n",
    "        for i, label in enumerate(labels)\n",
    "    ]\n",
    "\n",
    "for split in splits:\n",
    "    dataset_list = {}   # list of dicts per AR bucket, each containing list of {label=.., latent=..}\n",
    "    parts_uploaded = {}   # dict of int per AR bucket\n",
    "    dcae_batches = {}   # buffer, collect samples and batch process when full\n",
    "    samples_uploaded = 0\n",
    "\n",
    "    for i, d in tqdm(enumerate(ds[split]), total=len(ds[split]), desc=f\"Processing split {split}\"):\n",
    "        img=d[col_img]\n",
    "        label=d[col_label]\n",
    "        ar_bucket, img = resize(img, resizeTo=resizeTo, ARs=ASPECT_RATIOS)\n",
    "\n",
    "        # fill dcae-queue  \n",
    "        if not ar_bucket in dcae_batches: dcae_batches[ar_bucket]={\"labels\": [], \"images\": []}\n",
    "        dcae_batches[ar_bucket][\"labels\"].append(label)\n",
    "        dcae_batches[ar_bucket][\"images\"].append(img)\n",
    "        del ar_bucket\n",
    "\n",
    "        # process batch if full or at the end\n",
    "        ar_buckets = list(dcae_batches.keys())\n",
    "        for ar_bucket in ar_buckets:\n",
    "            target_split = f\"{split}_{ar_bucket}\"     # name of split the images of this batch belong to\n",
    "\n",
    "            if (\n",
    "                # batch is full -> process\n",
    "                (len(dcae_batches[ar_bucket][\"labels\"]) >= dcae_batch_size)\n",
    "                or \n",
    "                # batch is not full but we reached end of dataset -> process\n",
    "                (i == len(ds[split])-1 and len(dcae_batches[ar_bucket][\"labels\"]) > 0)\n",
    "            ):\n",
    "                if target_split not in dataset_list: \n",
    "                    dataset_list[target_split] = []                \n",
    "                latents = process_dcae_batch(dcae_batches[ar_bucket])\n",
    "                dataset_list[target_split].extend(latents)\n",
    "\n",
    "                # empty the dcae batch we just processed\n",
    "                dcae_batches[ar_bucket]={\"labels\": [], \"images\": []}\n",
    "\n",
    "        # upload to HF if we gathered more than upload_every OR reached the end \n",
    "        target_splits = list(dataset_list.keys())\n",
    "        for target_split in target_splits:\n",
    "            if (\n",
    "                # processed enough -> upload\n",
    "                (len(dataset_list[target_split]) >= upload_every)\n",
    "                or \n",
    "                # reached end of dataset -> upload\n",
    "                (i == len(ds[split])-1 and len(dataset_list[target_split]) > 0)\n",
    "            ):\n",
    "                if target_split not in parts_uploaded: \n",
    "                    parts_uploaded[target_split]=0\n",
    "                if not test_run:\n",
    "                    Dataset.from_list(dataset_list[target_split]).push_to_hub(\n",
    "                        hf_dataset, \n",
    "                        split=f\"{target_split}.part_{parts_uploaded[target_split]}\", \n",
    "                        num_shards=1\n",
    "                    )\n",
    "                parts_uploaded[target_split]+=1\n",
    "                samples_uploaded += len(dataset_list[target_split])\n",
    "                print(\"Uploaded\",len(dataset_list[target_split]), \"samples of split\", target_split, \"part\", parts_uploaded[target_split])\n",
    "                dataset_list[target_split]=[]   \n",
    "\n",
    "    print(\"split\", split, \"total samples uploaded:\", samples_uploaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402e335-a9b1-49dc-b4b2-a2277f97eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_list.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac57334-1ed9-4bd0-8805-33cf44ff0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check a few samples\n",
    "# num_samples = 12\n",
    "# dataset = dataset_list\n",
    "# for split in dataset:\n",
    "#     print(\"split\", split)\n",
    "#     for idx in [random.randint(0, len(dataset[split])-1) for _ in range(num_samples)]:\n",
    "#         latent = torch.Tensor(dataset[split][idx][\"latent\"])\n",
    "#         label = dataset[split][idx][\"label\"]\n",
    "#         print(label, latent.shape)\n",
    "#         display(\n",
    "#             make_grid(\n",
    "#                 [latent_to_PIL(latent.to(dcae.dtype).to(dcae.device), dcae)]\n",
    "#             )\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
