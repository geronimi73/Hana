{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "610d29a7-90d0-4aea-a535-56f2ee26145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U transformers accelerate datasets git+https://github.com/huggingface/diffusers Pillow==9.4.0 torchmetrics wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a076321-f9eb-4f7d-b3fd-a0b0b418d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from local_secrets import hf_token, wandb_key\n",
    "# from huggingface_hub import login\n",
    "# import wandb\n",
    "\n",
    "# login(token=hf_token)\n",
    "# wandb.login(key=wandb_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60395d2d-a5f3-432d-9d29-cc3e6adf8774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn.functional as F, random, wandb, time\n",
    "import torchvision.transforms as T\n",
    "from torchvision import transforms\n",
    "from diffusers import AutoencoderDC, SanaTransformer2DModel\n",
    "from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n",
    "from transformers import AutoModel, AutoTokenizer, set_seed\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import PIL_to_latent, latent_to_PIL, make_grid, encode_prompt, dcae_scalingf, pil_clipscore\n",
    "from utils import cifar10_labels\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "539a7f32-9159-48ca-9706-5b0566415fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g/.local/lib/python3.10/site-packages/diffusers/configuration_utils.py:245: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'diffusers.models.transformers.sana_transformer.SanaTransformer2DModel'>.load_config(...) followed by <class 'diffusers.models.transformers.sana_transformer.SanaTransformer2DModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.bfloat16\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "\n",
    "transformer = SanaTransformer2DModel.from_config(\"transformer_Sana-7L-MBERT_config.json\").to(device).to(dtype)\n",
    "text_encoder = AutoModel.from_pretrained(\"answerdotai/ModernBERT-base\", torch_dtype=dtype).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\", torch_dtype=dtype)\n",
    "\n",
    "model = \"Efficient-Large-Model/Sana_600M_1024px_diffusers\"\n",
    "dcae = AutoencoderDC.from_pretrained(model, subfolder=\"vae\", torch_dtype=dtype).to(device)\n",
    "scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2239874e-8912-4d43-a18b-bc764091aecc",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e806441-ef9c-4d81-82b8-4eac53550669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'latent'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"g-ronimo/CIFAR10-64-latents_dc-ae-f32c32-sana-1.0\")\n",
    "ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abaa1267-d803-4833-ae1e-b07205b7150e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2, torch.Size([1, 300, 768]), torch.Size([1, 300]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = cifar10_labels\n",
    "labels_encoded={k: encode_prompt(labels[k], tokenizer, text_encoder) for k in labels}\n",
    "\n",
    "len(labels_encoded), len(labels_encoded[0]), labels_encoded[0][0].shape, labels_encoded[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd3018cf-f8c3-4ed1-8c66-24644152b051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " tensor(-0.0366, device='cuda:0', dtype=torch.bfloat16),\n",
       " torch.Size([2, 32, 2, 2]),\n",
       " torch.Size([2, 300, 768]),\n",
       " torch.Size([2, 300]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate(items):\n",
    "    labels = [i[\"label\"] for i in items]\n",
    "    latents = torch.cat([torch.Tensor(i[\"latent\"]) for i in items]).to(dtype).to(device)\n",
    "    prompts_encoded = torch.cat([labels_encoded[label][0] for label in labels])\n",
    "    prompts_atnmask = torch.cat([labels_encoded[label][1] for label in labels])\n",
    "\n",
    "    return labels, latents, prompts_encoded, prompts_atnmask\n",
    "\n",
    "dataloader = DataLoader(ds[\"train\"], batch_size=2, shuffle=True, generator = torch.manual_seed(seed), collate_fn=collate)\n",
    "labels, latents, prompts_encoded, prompts_atnmask = next(iter(dataloader))\n",
    "len(labels), latents.mean(), latents.shape, prompts_encoded.shape, prompts_atnmask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb384ce1-f799-4cdd-8b0e-82c6422abde6",
   "metadata": {},
   "source": [
    "# Helpers for eval and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9310e27-651b-424b-ae73-f9203875dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.set_timesteps(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77a90e44-c1c3-4d53-967d-8c3158bcf484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.Image.Image image mode=RGB size=64x64>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(prompt, latent_dim=[1, 32, 2, 2], latent_seed=42):\n",
    "    scheduler.set_timesteps(scheduler.timesteps.size(0))     # reset step index\n",
    "    prompt_encoded, prompt_atnmask = encode_prompt(prompt, tokenizer, text_encoder)\n",
    "    latents = torch.randn(latent_dim, generator = torch.manual_seed(latent_seed)).to(dtype).to(device)\n",
    "\n",
    "    for t_idx in range(scheduler.timesteps.size(0)):\n",
    "        t = scheduler.timesteps[t_idx].unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            noise_pred = transformer(latents, encoder_hidden_states=prompt_encoded, timestep=t, encoder_attention_mask=prompt_atnmask, return_dict=False)[0]\n",
    "        latents = scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "    return latent_to_PIL(latents / dcae_scalingf, dcae)\n",
    "\n",
    "[generate(\"0\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "670f601e-5383-4f49-8d6f-a534f42e63ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 53.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.659375"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_loss(data_val, num_samples=10, batch_size=24):\n",
    "    losses = []\n",
    "    eval_dataloader = iter(DataLoader(data_val, batch_size=batch_size, shuffle=False, collate_fn=collate))\n",
    "    \n",
    "    for i in tqdm(range(num_samples), \"eval_loss\"):\n",
    "        label, latent, prompt_encoded, prompt_atnmask = next(eval_dataloader)\n",
    "        noise = torch.randn_like(latent)\n",
    "        timestep = scheduler.timesteps[torch.randint(scheduler.timesteps.size(0),(latent.shape[0],))].to(device)\n",
    "        latent_noisy = scheduler.scale_noise(latent, timestep, noise)\n",
    "        with torch.no_grad():\n",
    "            noise_pred = transformer(latent_noisy, encoder_hidden_states = prompt_encoded, encoder_attention_mask = prompt_atnmask, timestep = timestep, return_dict=False)[0]\n",
    "        loss = F.mse_loss(noise_pred, noise - latent)\n",
    "        losses.append(loss.item())\n",
    "    return sum(losses)/len(losses)\n",
    "\n",
    "eval_loss(ds[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5748ec87-d482-49c0-bf22-c1336eb71584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_images: 100%|██████████████████████████████| 10/10 [00:08<00:00,  1.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21.688941955566406"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_clipscore(images):\n",
    "    prompts = [cifar10_labels[k] for k in cifar10_labels]\n",
    "    return pil_clipscore(images, prompts)\n",
    "\n",
    "scheduler.set_timesteps(100)\n",
    "images = [generate(p) for p in tqdm([cifar10_labels[k] for k in cifar10_labels], \"eval_images\")]\n",
    "eval_clipscore(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d9761-8eaa-4f23-bfad-cc5fa3ce923f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "413d4b4c-1efe-48f6-b422-40340a4718e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 156.41M\n",
      "2 splits: ['train', 'test'] [50000, 10000]\n"
     ]
    }
   ],
   "source": [
    "log_wandb = True\n",
    "lr = 5e-4\n",
    "# bs = 64\n",
    "bs = 896\n",
    "epochs = 500\n",
    "# timesteps_training = 10\n",
    "# timesteps_generate = 10\n",
    "timesteps_training = 1000\n",
    "timesteps_generate = 1000\n",
    "steps_log, steps_eval = 10, 200\n",
    "# steps_log, steps_eval = 10, 20\n",
    "\n",
    "splits = list(ds.keys())\n",
    "data_train, data_val = ds[splits[0]], ds[splits[1]]\n",
    "\n",
    "steps_epoch = len(data_train) // bs\n",
    "\n",
    "dataloader = DataLoader(data_train, batch_size=bs, shuffle=True, generator = torch.manual_seed(seed), collate_fn=collate)\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=lr)\n",
    "scheduler.set_timesteps(timesteps_training)\n",
    "\n",
    "model_size = sum(p.numel() for p in transformer.parameters() if p.requires_grad)\n",
    "print(f\"Model parameters: {model_size / 1e6:.2f}M\")\n",
    "print(f\"{len(splits)} splits: {splits}\", [len(ds[s]) for s in splits])\n",
    "assert len(splits)==2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1815d77d-33fd-40f2-aba1-08f28d8adce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mg-ronimo\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/g/jupyter/2025-01-30_Hana-Alpha11-CIFAR10-DDP/wandb/run-20250201_001300-14b0swrd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/g-ronimo/Hana/runs/14b0swrd' target=\"_blank\">Z-156.41M_CIFAR10_LR-0.0005_BS-896_TS-1000_my3090</a></strong> to <a href='https://wandb.ai/g-ronimo/Hana' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/g-ronimo/Hana' target=\"_blank\">https://wandb.ai/g-ronimo/Hana</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/g-ronimo/Hana/runs/14b0swrd' target=\"_blank\">https://wandb.ai/g-ronimo/Hana/runs/14b0swrd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 57.58it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:24<00:00,  8.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, eval loss: 11.0687, clipscore: 22.85\n",
      "step 10, epoch: 0.1818, train loss: 5.2188, grad_norm: 3.45, 10063.24ms/step, 89.04samples/sec\n",
      "step 20, epoch: 0.3636, train loss: 4.1562, grad_norm: 2.94, 1065.73ms/step, 840.74samples/sec\n",
      "step 30, epoch: 0.5455, train loss: 3.5469, grad_norm: 2.23, 1087.95ms/step, 823.57samples/sec\n",
      "step 40, epoch: 0.7273, train loss: 3.3281, grad_norm: 2.41, 1036.00ms/step, 864.87samples/sec\n",
      "step 50, epoch: 0.9091, train loss: 3.3750, grad_norm: 2.23, 1067.88ms/step, 839.04samples/sec\n",
      "step 60, epoch: 1.0909, train loss: 3.2188, grad_norm: 1.59, 1017.76ms/step, 880.36samples/sec\n",
      "step 70, epoch: 1.2727, train loss: 3.1250, grad_norm: 1.51, 1034.63ms/step, 866.01samples/sec\n",
      "step 80, epoch: 1.4545, train loss: 3.1250, grad_norm: 1.41, 1070.67ms/step, 836.86samples/sec\n",
      "step 90, epoch: 1.6364, train loss: 3.0781, grad_norm: 1.30, 1029.84ms/step, 870.04samples/sec\n",
      "step 100, epoch: 1.8182, train loss: 3.1094, grad_norm: 1.26, 1070.81ms/step, 836.75samples/sec\n",
      "step 110, epoch: 2.0000, train loss: 3.0938, grad_norm: 1.21, 1036.16ms/step, 864.73samples/sec\n",
      "step 120, epoch: 2.1818, train loss: 3.0781, grad_norm: 1.09, 1019.63ms/step, 878.75samples/sec\n",
      "step 130, epoch: 2.3636, train loss: 3.0469, grad_norm: 1.12, 1063.96ms/step, 842.13samples/sec\n",
      "step 140, epoch: 2.5455, train loss: 3.0156, grad_norm: 1.24, 1031.23ms/step, 868.87samples/sec\n",
      "step 150, epoch: 2.7273, train loss: 2.9531, grad_norm: 1.01, 1030.39ms/step, 869.57samples/sec\n",
      "step 160, epoch: 2.9091, train loss: 3.0000, grad_norm: 0.89, 1066.23ms/step, 840.35samples/sec\n",
      "step 170, epoch: 3.0909, train loss: 2.9531, grad_norm: 0.91, 1011.73ms/step, 885.61samples/sec\n",
      "step 180, epoch: 3.2727, train loss: 3.0312, grad_norm: 1.09, 1066.64ms/step, 840.02samples/sec\n",
      "step 190, epoch: 3.4545, train loss: 3.0312, grad_norm: 0.84, 1053.20ms/step, 850.74samples/sec\n",
      "step 200, epoch: 3.6364, train loss: 3.0312, grad_norm: 0.96, 1037.22ms/step, 863.84samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 58.75it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:24<00:00,  8.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200, eval loss: 3.0406, clipscore: 22.72\n",
      "step 210, epoch: 3.8182, train loss: 2.8906, grad_norm: 0.99, 9953.49ms/step, 90.02samples/sec\n",
      "step 220, epoch: 4.0000, train loss: 2.9219, grad_norm: 0.93, 1045.79ms/step, 856.77samples/sec\n",
      "step 230, epoch: 4.1818, train loss: 2.9844, grad_norm: 0.86, 1052.12ms/step, 851.61samples/sec\n",
      "step 240, epoch: 4.3636, train loss: 2.9844, grad_norm: 0.86, 1034.02ms/step, 866.52samples/sec\n",
      "step 250, epoch: 4.5455, train loss: 3.0000, grad_norm: 0.73, 1033.18ms/step, 867.22samples/sec\n",
      "step 260, epoch: 4.7273, train loss: 2.9844, grad_norm: 0.65, 1065.32ms/step, 841.07samples/sec\n",
      "step 270, epoch: 4.9091, train loss: 2.9531, grad_norm: 0.89, 1028.46ms/step, 871.21samples/sec\n",
      "step 280, epoch: 5.0909, train loss: 2.9531, grad_norm: 0.80, 1046.80ms/step, 855.95samples/sec\n",
      "step 290, epoch: 5.2727, train loss: 2.9062, grad_norm: 0.75, 1065.39ms/step, 841.01samples/sec\n",
      "step 300, epoch: 5.4545, train loss: 2.9844, grad_norm: 0.77, 1036.11ms/step, 864.77samples/sec\n",
      "step 310, epoch: 5.6364, train loss: 2.8594, grad_norm: 0.70, 1069.64ms/step, 837.67samples/sec\n",
      "step 320, epoch: 5.8182, train loss: 2.9844, grad_norm: 0.71, 1033.99ms/step, 866.54samples/sec\n",
      "step 330, epoch: 6.0000, train loss: 2.9531, grad_norm: 0.79, 1070.76ms/step, 836.79samples/sec\n",
      "step 340, epoch: 6.1818, train loss: 3.0156, grad_norm: 0.73, 1015.66ms/step, 882.18samples/sec\n",
      "step 350, epoch: 6.3636, train loss: 2.8594, grad_norm: 0.71, 1036.69ms/step, 864.29samples/sec\n",
      "step 360, epoch: 6.5455, train loss: 2.9062, grad_norm: 0.61, 1071.16ms/step, 836.48samples/sec\n",
      "step 370, epoch: 6.7273, train loss: 2.8906, grad_norm: 0.66, 1031.73ms/step, 868.45samples/sec\n",
      "step 380, epoch: 6.9091, train loss: 2.9375, grad_norm: 0.65, 1069.20ms/step, 838.01samples/sec\n",
      "step 390, epoch: 7.0909, train loss: 2.9375, grad_norm: 0.65, 1036.97ms/step, 864.05samples/sec\n",
      "step 400, epoch: 7.2727, train loss: 2.9062, grad_norm: 0.55, 1020.88ms/step, 877.67samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 59.01it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:22<00:00,  8.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400, eval loss: 2.8609, clipscore: 23.07\n",
      "step 410, epoch: 7.4545, train loss: 2.8281, grad_norm: 0.75, 9611.19ms/step, 93.22samples/sec\n",
      "step 420, epoch: 7.6364, train loss: 2.8594, grad_norm: 0.65, 1027.16ms/step, 872.31samples/sec\n",
      "step 430, epoch: 7.8182, train loss: 2.9531, grad_norm: 0.59, 1025.72ms/step, 873.53samples/sec\n",
      "step 440, epoch: 8.0000, train loss: 2.8594, grad_norm: 0.58, 1063.22ms/step, 842.73samples/sec\n",
      "step 450, epoch: 8.1818, train loss: 2.9375, grad_norm: 0.69, 1010.48ms/step, 886.71samples/sec\n",
      "step 460, epoch: 8.3636, train loss: 2.9531, grad_norm: 0.67, 1027.74ms/step, 871.82samples/sec\n",
      "step 470, epoch: 8.5455, train loss: 2.9062, grad_norm: 0.64, 1060.72ms/step, 844.71samples/sec\n",
      "step 480, epoch: 8.7273, train loss: 2.9375, grad_norm: 0.57, 1030.86ms/step, 869.17samples/sec\n",
      "step 490, epoch: 8.9091, train loss: 2.9062, grad_norm: 0.61, 1059.90ms/step, 845.36samples/sec\n",
      "step 500, epoch: 9.0909, train loss: 2.9531, grad_norm: 0.59, 1030.33ms/step, 869.62samples/sec\n",
      "step 510, epoch: 9.2727, train loss: 2.8438, grad_norm: 0.54, 1012.30ms/step, 885.11samples/sec\n",
      "step 520, epoch: 9.4545, train loss: 2.9531, grad_norm: 0.53, 1101.29ms/step, 813.59samples/sec\n",
      "step 530, epoch: 9.6364, train loss: 2.9219, grad_norm: 0.57, 1068.36ms/step, 838.66samples/sec\n",
      "step 540, epoch: 9.8182, train loss: 2.8750, grad_norm: 0.68, 1070.87ms/step, 836.70samples/sec\n",
      "step 550, epoch: 10.0000, train loss: 2.9219, grad_norm: 0.74, 1118.88ms/step, 800.80samples/sec\n",
      "step 560, epoch: 10.1818, train loss: 2.8906, grad_norm: 0.61, 1051.48ms/step, 852.13samples/sec\n",
      "step 570, epoch: 10.3636, train loss: 2.8750, grad_norm: 0.48, 1071.70ms/step, 836.06samples/sec\n",
      "step 580, epoch: 10.5455, train loss: 2.9375, grad_norm: 0.56, 1034.82ms/step, 865.85samples/sec\n",
      "step 590, epoch: 10.7273, train loss: 2.9219, grad_norm: 0.53, 1033.45ms/step, 867.00samples/sec\n",
      "step 600, epoch: 10.9091, train loss: 2.9844, grad_norm: 0.62, 1071.04ms/step, 836.57samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 57.55it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:26<00:00,  8.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 600, eval loss: 2.9562, clipscore: 23.08\n",
      "step 610, epoch: 11.0909, train loss: 2.8438, grad_norm: 0.64, 10013.09ms/step, 89.48samples/sec\n",
      "step 620, epoch: 11.2727, train loss: 2.9531, grad_norm: 0.56, 1075.62ms/step, 833.01samples/sec\n",
      "step 630, epoch: 11.4545, train loss: 2.8906, grad_norm: 0.60, 1058.25ms/step, 846.68samples/sec\n",
      "step 640, epoch: 11.6364, train loss: 2.9219, grad_norm: 0.59, 1060.51ms/step, 844.87samples/sec\n",
      "step 650, epoch: 11.8182, train loss: 2.9062, grad_norm: 0.58, 1103.55ms/step, 811.92samples/sec\n",
      "step 660, epoch: 12.0000, train loss: 2.9375, grad_norm: 0.54, 1061.64ms/step, 843.98samples/sec\n",
      "step 670, epoch: 12.1818, train loss: 2.8594, grad_norm: 0.56, 1107.52ms/step, 809.02samples/sec\n",
      "step 680, epoch: 12.3636, train loss: 2.9062, grad_norm: 0.62, 1043.98ms/step, 858.26samples/sec\n",
      "step 690, epoch: 12.5455, train loss: 2.8594, grad_norm: 0.53, 1073.14ms/step, 834.93samples/sec\n",
      "step 700, epoch: 12.7273, train loss: 2.8594, grad_norm: 0.47, 1111.69ms/step, 805.98samples/sec\n",
      "step 710, epoch: 12.9091, train loss: 2.8438, grad_norm: 0.64, 1067.61ms/step, 839.26samples/sec\n",
      "step 720, epoch: 13.0909, train loss: 2.9062, grad_norm: 0.58, 1112.30ms/step, 805.54samples/sec\n",
      "step 730, epoch: 13.2727, train loss: 2.8750, grad_norm: 0.54, 1049.54ms/step, 853.71samples/sec\n",
      "step 740, epoch: 13.4545, train loss: 2.9375, grad_norm: 0.61, 1068.22ms/step, 838.78samples/sec\n",
      "step 750, epoch: 13.6364, train loss: 2.8438, grad_norm: 0.61, 1114.53ms/step, 803.93samples/sec\n",
      "step 760, epoch: 13.8182, train loss: 2.8750, grad_norm: 0.50, 1069.49ms/step, 837.78samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_images: 100%|██████████████████████████████| 10/10 [01:22<00:00,  8.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 800, eval loss: 2.8406, clipscore: 22.59\n",
      "step 810, epoch: 14.7273, train loss: 2.7812, grad_norm: 0.52, 9602.72ms/step, 93.31samples/sec\n",
      "step 820, epoch: 14.9091, train loss: 2.7969, grad_norm: 0.54, 1028.32ms/step, 871.32samples/sec\n",
      "step 830, epoch: 15.0909, train loss: 2.8750, grad_norm: 0.56, 1060.27ms/step, 845.06samples/sec\n",
      "step 840, epoch: 15.2727, train loss: 2.8594, grad_norm: 0.48, 1011.95ms/step, 885.42samples/sec\n",
      "step 850, epoch: 15.4545, train loss: 2.9219, grad_norm: 0.59, 1064.89ms/step, 841.40samples/sec\n",
      "step 860, epoch: 15.6364, train loss: 2.9062, grad_norm: 0.52, 1032.06ms/step, 868.16samples/sec\n",
      "step 870, epoch: 15.8182, train loss: 2.8125, grad_norm: 0.57, 1067.62ms/step, 839.25samples/sec\n",
      "step 880, epoch: 16.0000, train loss: 2.8594, grad_norm: 0.54, 1071.01ms/step, 836.59samples/sec\n",
      "step 890, epoch: 16.1818, train loss: 2.8438, grad_norm: 0.52, 1032.46ms/step, 867.83samples/sec\n",
      "step 900, epoch: 16.3636, train loss: 2.8750, grad_norm: 0.54, 1046.21ms/step, 856.42samples/sec\n",
      "step 910, epoch: 16.5455, train loss: 2.8125, grad_norm: 0.54, 1036.83ms/step, 864.17samples/sec\n",
      "step 920, epoch: 16.7273, train loss: 2.9219, grad_norm: 0.53, 1035.66ms/step, 865.15samples/sec\n",
      "step 930, epoch: 16.9091, train loss: 2.8438, grad_norm: 0.51, 1067.06ms/step, 839.69samples/sec\n",
      "step 940, epoch: 17.0909, train loss: 2.8281, grad_norm: 0.62, 1036.19ms/step, 864.71samples/sec\n",
      "step 950, epoch: 17.2727, train loss: 2.8750, grad_norm: 0.61, 1067.44ms/step, 839.39samples/sec\n",
      "step 960, epoch: 17.4545, train loss: 2.8281, grad_norm: 0.50, 1008.79ms/step, 888.20samples/sec\n",
      "step 970, epoch: 17.6364, train loss: 2.8438, grad_norm: 0.49, 1034.85ms/step, 865.83samples/sec\n",
      "step 980, epoch: 17.8182, train loss: 2.9219, grad_norm: 0.54, 1070.27ms/step, 837.17samples/sec\n",
      "step 990, epoch: 18.0000, train loss: 2.9375, grad_norm: 0.50, 1035.56ms/step, 865.23samples/sec\n",
      "step 1000, epoch: 18.1818, train loss: 2.9375, grad_norm: 0.49, 1066.88ms/step, 839.83samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 58.41it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:25<00:00,  8.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000, eval loss: 2.9609, clipscore: 23.70\n",
      "step 1010, epoch: 18.3636, train loss: 2.8750, grad_norm: 0.49, 9869.08ms/step, 90.79samples/sec\n",
      "step 1020, epoch: 18.5455, train loss: 2.9219, grad_norm: 0.52, 1058.27ms/step, 846.67samples/sec\n",
      "step 1030, epoch: 18.7273, train loss: 2.8750, grad_norm: 0.53, 1102.00ms/step, 813.06samples/sec\n",
      "step 1040, epoch: 18.9091, train loss: 2.8750, grad_norm: 0.50, 1058.24ms/step, 846.68samples/sec\n",
      "step 1050, epoch: 19.0909, train loss: 2.8438, grad_norm: 0.55, 1100.07ms/step, 814.49samples/sec\n",
      "step 1060, epoch: 19.2727, train loss: 2.9062, grad_norm: 0.47, 1057.94ms/step, 846.93samples/sec\n",
      "step 1070, epoch: 19.4545, train loss: 2.9375, grad_norm: 0.57, 1044.77ms/step, 857.60samples/sec\n",
      "step 1080, epoch: 19.6364, train loss: 2.8594, grad_norm: 0.49, 1109.84ms/step, 807.33samples/sec\n",
      "step 1090, epoch: 19.8182, train loss: 2.8281, grad_norm: 0.44, 1066.91ms/step, 839.81samples/sec\n",
      "step 1100, epoch: 20.0000, train loss: 2.8438, grad_norm: 0.50, 1109.94ms/step, 807.25samples/sec\n",
      "step 1110, epoch: 20.1818, train loss: 2.7500, grad_norm: 0.54, 1040.60ms/step, 861.05samples/sec\n",
      "step 1120, epoch: 20.3636, train loss: 2.8438, grad_norm: 0.52, 1013.07ms/step, 884.44samples/sec\n",
      "step 1130, epoch: 20.5455, train loss: 2.8594, grad_norm: 0.56, 1070.52ms/step, 836.98samples/sec\n",
      "step 1140, epoch: 20.7273, train loss: 2.8750, grad_norm: 0.46, 1036.85ms/step, 864.16samples/sec\n",
      "step 1150, epoch: 20.9091, train loss: 2.8125, grad_norm: 0.49, 1035.49ms/step, 865.29samples/sec\n",
      "step 1160, epoch: 21.0909, train loss: 2.8281, grad_norm: 0.53, 1072.91ms/step, 835.11samples/sec\n",
      "step 1170, epoch: 21.2727, train loss: 2.8438, grad_norm: 0.51, 1032.90ms/step, 867.46samples/sec\n",
      "step 1180, epoch: 21.4545, train loss: 2.9219, grad_norm: 0.54, 1052.43ms/step, 851.37samples/sec\n",
      "step 1190, epoch: 21.6364, train loss: 2.8281, grad_norm: 0.46, 1035.99ms/step, 864.87samples/sec\n",
      "step 1200, epoch: 21.8182, train loss: 2.8594, grad_norm: 0.53, 1035.26ms/step, 865.48samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 58.39it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:23<00:00,  8.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1200, eval loss: 2.8109, clipscore: 22.93\n",
      "step 1210, epoch: 22.0000, train loss: 2.7812, grad_norm: 0.53, 9804.40ms/step, 91.39samples/sec\n",
      "step 1220, epoch: 22.1818, train loss: 2.8125, grad_norm: 0.48, 1084.98ms/step, 825.82samples/sec\n",
      "step 1230, epoch: 22.3636, train loss: 2.8125, grad_norm: 0.47, 1103.29ms/step, 812.11samples/sec\n",
      "step 1240, epoch: 22.5455, train loss: 2.8750, grad_norm: 0.50, 1033.14ms/step, 867.26samples/sec\n",
      "step 1250, epoch: 22.7273, train loss: 2.8281, grad_norm: 0.51, 1062.59ms/step, 843.23samples/sec\n",
      "step 1260, epoch: 22.9091, train loss: 2.9062, grad_norm: 0.57, 1108.42ms/step, 808.36samples/sec\n",
      "step 1270, epoch: 23.0909, train loss: 2.8125, grad_norm: 0.51, 1033.16ms/step, 867.24samples/sec\n",
      "step 1280, epoch: 23.2727, train loss: 2.8594, grad_norm: 0.48, 1032.59ms/step, 867.72samples/sec\n",
      "step 1290, epoch: 23.4545, train loss: 2.8125, grad_norm: 0.47, 1044.52ms/step, 857.81samples/sec\n",
      "step 1300, epoch: 23.6364, train loss: 2.8438, grad_norm: 0.47, 1031.03ms/step, 869.03samples/sec\n",
      "step 1310, epoch: 23.8182, train loss: 2.7812, grad_norm: 0.47, 1066.83ms/step, 839.87samples/sec\n",
      "step 1320, epoch: 24.0000, train loss: 2.8438, grad_norm: 0.54, 1034.01ms/step, 866.53samples/sec\n",
      "step 1330, epoch: 24.1818, train loss: 2.8594, grad_norm: 0.54, 1034.45ms/step, 866.16samples/sec\n",
      "step 1340, epoch: 24.3636, train loss: 2.8594, grad_norm: 0.53, 1107.13ms/step, 809.30samples/sec\n",
      "step 1350, epoch: 24.5455, train loss: 2.8125, grad_norm: 0.45, 1045.52ms/step, 856.99samples/sec\n",
      "step 1360, epoch: 24.7273, train loss: 2.8125, grad_norm: 0.49, 1112.77ms/step, 805.20samples/sec\n",
      "step 1370, epoch: 24.9091, train loss: 2.7500, grad_norm: 0.46, 1068.92ms/step, 838.23samples/sec\n",
      "step 1380, epoch: 25.0909, train loss: 2.8594, grad_norm: 0.50, 1064.83ms/step, 841.45samples/sec\n",
      "step 1390, epoch: 25.2727, train loss: 2.9219, grad_norm: 0.47, 1111.13ms/step, 806.38samples/sec\n",
      "step 1400, epoch: 25.4545, train loss: 2.7969, grad_norm: 0.47, 1045.69ms/step, 856.85samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 57.83it/s]\n",
      "eval_images:   0%|                                       | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1420, epoch: 25.8182, train loss: 2.7344, grad_norm: 0.42, 1040.20ms/step, 861.37samples/sec\n",
      "step 1430, epoch: 26.0000, train loss: 2.8125, grad_norm: 0.44, 1028.97ms/step, 870.78samples/sec\n",
      "step 1440, epoch: 26.1818, train loss: 2.7969, grad_norm: 0.48, 1061.23ms/step, 844.30samples/sec\n",
      "step 1450, epoch: 26.3636, train loss: 2.9062, grad_norm: 0.47, 1028.02ms/step, 871.58samples/sec\n",
      "step 1460, epoch: 26.5455, train loss: 2.8438, grad_norm: 0.46, 1047.39ms/step, 855.46samples/sec\n",
      "step 1470, epoch: 26.7273, train loss: 2.7969, grad_norm: 0.52, 1031.83ms/step, 868.36samples/sec\n",
      "step 1480, epoch: 26.9091, train loss: 2.8125, grad_norm: 0.46, 1044.65ms/step, 857.70samples/sec\n",
      "step 1490, epoch: 27.0909, train loss: 2.7969, grad_norm: 0.43, 1106.25ms/step, 809.94samples/sec\n",
      "step 1500, epoch: 27.2727, train loss: 2.8594, grad_norm: 0.47, 1059.94ms/step, 845.33samples/sec\n",
      "step 1510, epoch: 27.4545, train loss: 2.8594, grad_norm: 0.51, 1105.60ms/step, 810.42samples/sec\n",
      "step 1520, epoch: 27.6364, train loss: 2.8594, grad_norm: 0.54, 1044.12ms/step, 858.14samples/sec\n",
      "step 1530, epoch: 27.8182, train loss: 2.8125, grad_norm: 0.47, 1034.26ms/step, 866.32samples/sec\n",
      "step 1540, epoch: 28.0000, train loss: 2.7812, grad_norm: 0.52, 1065.02ms/step, 841.30samples/sec\n",
      "step 1550, epoch: 28.1818, train loss: 2.7500, grad_norm: 0.44, 1046.79ms/step, 855.95samples/sec\n",
      "step 1560, epoch: 28.3636, train loss: 2.7969, grad_norm: 0.52, 1032.61ms/step, 867.70samples/sec\n",
      "step 1570, epoch: 28.5455, train loss: 2.7500, grad_norm: 0.46, 1042.67ms/step, 859.33samples/sec\n",
      "step 1580, epoch: 28.7273, train loss: 2.8125, grad_norm: 0.53, 1030.64ms/step, 869.36samples/sec\n",
      "step 1590, epoch: 28.9091, train loss: 2.8438, grad_norm: 0.50, 1067.91ms/step, 839.02samples/sec\n",
      "step 1600, epoch: 29.0909, train loss: 2.8594, grad_norm: 0.49, 1030.51ms/step, 869.47samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 59.70it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:23<00:00,  8.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1600, eval loss: 2.9516, clipscore: 23.34\n",
      "step 1610, epoch: 29.2727, train loss: 2.7031, grad_norm: 0.53, 9736.19ms/step, 92.03samples/sec\n",
      "step 1620, epoch: 29.4545, train loss: 2.7344, grad_norm: 0.42, 1096.03ms/step, 817.50samples/sec\n",
      "step 1630, epoch: 29.6364, train loss: 2.8281, grad_norm: 0.47, 1037.84ms/step, 863.33samples/sec\n",
      "step 1640, epoch: 29.8182, train loss: 2.8125, grad_norm: 0.46, 1102.72ms/step, 812.54samples/sec\n",
      "step 1650, epoch: 30.0000, train loss: 2.8281, grad_norm: 0.53, 1068.16ms/step, 838.83samples/sec\n",
      "step 1660, epoch: 30.1818, train loss: 2.8125, grad_norm: 0.50, 1040.95ms/step, 860.75samples/sec\n",
      "step 1670, epoch: 30.3636, train loss: 2.7812, grad_norm: 0.55, 1066.63ms/step, 840.03samples/sec\n",
      "step 1680, epoch: 30.5455, train loss: 2.7812, grad_norm: 0.48, 1009.51ms/step, 887.56samples/sec\n",
      "step 1690, epoch: 30.7273, train loss: 2.7500, grad_norm: 0.47, 1040.92ms/step, 860.77samples/sec\n",
      "step 1700, epoch: 30.9091, train loss: 2.8125, grad_norm: 0.45, 1060.20ms/step, 845.12samples/sec\n",
      "step 1710, epoch: 31.0909, train loss: 2.7188, grad_norm: 0.51, 1061.54ms/step, 844.05samples/sec\n",
      "step 1720, epoch: 31.2727, train loss: 2.8281, grad_norm: 0.45, 1105.76ms/step, 810.30samples/sec\n",
      "step 1730, epoch: 31.4545, train loss: 2.7812, grad_norm: 0.46, 1065.42ms/step, 840.98samples/sec\n",
      "step 1740, epoch: 31.6364, train loss: 2.8594, grad_norm: 0.48, 1069.59ms/step, 837.71samples/sec\n",
      "step 1750, epoch: 31.8182, train loss: 2.7188, grad_norm: 0.54, 1030.77ms/step, 869.25samples/sec\n",
      "step 1760, epoch: 32.0000, train loss: 2.7344, grad_norm: 0.45, 1032.62ms/step, 867.69samples/sec\n",
      "step 1770, epoch: 32.1818, train loss: 2.7344, grad_norm: 0.46, 1064.54ms/step, 841.68samples/sec\n",
      "step 1780, epoch: 32.3636, train loss: 2.7969, grad_norm: 0.52, 1030.23ms/step, 869.71samples/sec\n",
      "step 1790, epoch: 32.5455, train loss: 2.7969, grad_norm: 0.46, 1067.36ms/step, 839.46samples/sec\n",
      "step 1800, epoch: 32.7273, train loss: 2.7656, grad_norm: 0.51, 1012.39ms/step, 885.04samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 58.34it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:24<00:00,  8.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1800, eval loss: 2.8250, clipscore: 22.84\n",
      "step 1810, epoch: 32.9091, train loss: 2.6875, grad_norm: 0.55, 9840.91ms/step, 91.05samples/sec\n",
      "step 1820, epoch: 33.0909, train loss: 2.7031, grad_norm: 0.45, 1060.79ms/step, 844.66samples/sec\n",
      "step 1830, epoch: 33.2727, train loss: 2.8281, grad_norm: 0.43, 1027.47ms/step, 872.04samples/sec\n",
      "step 1840, epoch: 33.4545, train loss: 2.7188, grad_norm: 0.45, 1029.03ms/step, 870.72samples/sec\n",
      "step 1850, epoch: 33.6364, train loss: 2.7969, grad_norm: 0.48, 1041.23ms/step, 860.52samples/sec\n",
      "step 1860, epoch: 33.8182, train loss: 2.7969, grad_norm: 0.48, 1030.47ms/step, 869.51samples/sec\n",
      "step 1870, epoch: 34.0000, train loss: 2.7500, grad_norm: 0.57, 1061.84ms/step, 843.82samples/sec\n",
      "step 1880, epoch: 34.1818, train loss: 2.7969, grad_norm: 0.45, 1030.06ms/step, 869.85samples/sec\n",
      "step 1890, epoch: 34.3636, train loss: 2.7812, grad_norm: 0.49, 1029.22ms/step, 870.56samples/sec\n",
      "step 1900, epoch: 34.5455, train loss: 2.8125, grad_norm: 0.45, 1063.89ms/step, 842.19samples/sec\n",
      "step 1910, epoch: 34.7273, train loss: 2.7031, grad_norm: 0.44, 1011.67ms/step, 885.67samples/sec\n",
      "step 1920, epoch: 34.9091, train loss: 2.8125, grad_norm: 0.48, 1063.64ms/step, 842.39samples/sec\n",
      "step 1930, epoch: 35.0909, train loss: 2.7812, grad_norm: 0.53, 1041.88ms/step, 859.98samples/sec\n",
      "step 1940, epoch: 35.2727, train loss: 2.7344, grad_norm: 0.56, 1064.12ms/step, 842.01samples/sec\n",
      "step 1950, epoch: 35.4545, train loss: 2.7812, grad_norm: 0.59, 1071.47ms/step, 836.23samples/sec\n",
      "step 1960, epoch: 35.6364, train loss: 2.7656, grad_norm: 0.44, 1011.16ms/step, 886.11samples/sec\n",
      "step 1970, epoch: 35.8182, train loss: 2.7344, grad_norm: 0.49, 1033.89ms/step, 866.63samples/sec\n",
      "step 1980, epoch: 36.0000, train loss: 2.7969, grad_norm: 0.48, 1031.29ms/step, 868.81samples/sec\n",
      "step 1990, epoch: 36.1818, train loss: 2.7812, grad_norm: 0.42, 1032.34ms/step, 867.93samples/sec\n",
      "step 2000, epoch: 36.3636, train loss: 2.8438, grad_norm: 0.45, 1067.36ms/step, 839.46samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 61.08it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:25<00:00,  8.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000, eval loss: 2.9594, clipscore: 23.09\n",
      "step 2010, epoch: 36.5455, train loss: 2.7031, grad_norm: 0.52, 9857.00ms/step, 90.90samples/sec\n",
      "step 2020, epoch: 36.7273, train loss: 2.8125, grad_norm: 0.45, 1038.49ms/step, 862.79samples/sec\n",
      "step 2030, epoch: 36.9091, train loss: 2.7500, grad_norm: 0.46, 1026.93ms/step, 872.50samples/sec\n",
      "step 2040, epoch: 37.0909, train loss: 2.7812, grad_norm: 0.50, 1025.89ms/step, 873.38samples/sec\n",
      "step 2050, epoch: 37.2727, train loss: 2.7812, grad_norm: 0.49, 1062.21ms/step, 843.52samples/sec\n",
      "step 2060, epoch: 37.4545, train loss: 2.7812, grad_norm: 0.47, 1026.34ms/step, 873.01samples/sec\n",
      "step 2070, epoch: 37.6364, train loss: 2.7188, grad_norm: 0.50, 1065.44ms/step, 840.96samples/sec\n",
      "step 2080, epoch: 37.8182, train loss: 2.7656, grad_norm: 0.50, 1005.18ms/step, 891.39samples/sec\n",
      "step 2090, epoch: 38.0000, train loss: 2.7344, grad_norm: 0.51, 1048.32ms/step, 854.70samples/sec\n",
      "step 2100, epoch: 38.1818, train loss: 2.7188, grad_norm: 0.48, 1107.74ms/step, 808.86samples/sec\n",
      "step 2110, epoch: 38.3636, train loss: 2.7031, grad_norm: 0.52, 1063.87ms/step, 842.21samples/sec\n",
      "step 2120, epoch: 38.5455, train loss: 2.7812, grad_norm: 0.56, 1042.40ms/step, 859.55samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_images: 100%|██████████████████████████████| 10/10 [01:24<00:00,  8.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2200, eval loss: 2.8578, clipscore: 23.05\n",
      "step 2210, epoch: 40.1818, train loss: 2.6406, grad_norm: 0.46, 9790.59ms/step, 91.52samples/sec\n",
      "step 2220, epoch: 40.3636, train loss: 2.6875, grad_norm: 0.47, 1028.89ms/step, 870.84samples/sec\n",
      "step 2230, epoch: 40.5455, train loss: 2.7500, grad_norm: 0.54, 1064.35ms/step, 841.83samples/sec\n",
      "step 2240, epoch: 40.7273, train loss: 2.7188, grad_norm: 0.46, 1004.98ms/step, 891.56samples/sec\n",
      "step 2250, epoch: 40.9091, train loss: 2.7656, grad_norm: 0.52, 1029.50ms/step, 870.33samples/sec\n",
      "step 2260, epoch: 41.0909, train loss: 2.7500, grad_norm: 0.47, 1066.33ms/step, 840.27samples/sec\n",
      "step 2270, epoch: 41.2727, train loss: 2.6875, grad_norm: 0.51, 1030.22ms/step, 869.71samples/sec\n",
      "step 2280, epoch: 41.4545, train loss: 2.7344, grad_norm: 0.50, 1034.17ms/step, 866.39samples/sec\n",
      "step 2290, epoch: 41.6364, train loss: 2.7031, grad_norm: 0.46, 1030.76ms/step, 869.26samples/sec\n",
      "step 2300, epoch: 41.8182, train loss: 2.7344, grad_norm: 0.49, 1011.13ms/step, 886.14samples/sec\n",
      "step 2310, epoch: 42.0000, train loss: 2.6719, grad_norm: 0.50, 1030.59ms/step, 869.41samples/sec\n",
      "step 2320, epoch: 42.1818, train loss: 2.7812, grad_norm: 0.48, 1031.75ms/step, 868.43samples/sec\n",
      "step 2330, epoch: 42.3636, train loss: 2.7031, grad_norm: 0.49, 1066.94ms/step, 839.79samples/sec\n",
      "step 2340, epoch: 42.5455, train loss: 2.7031, grad_norm: 0.64, 1032.28ms/step, 867.98samples/sec\n",
      "step 2350, epoch: 42.7273, train loss: 2.7500, grad_norm: 0.55, 1064.90ms/step, 841.39samples/sec\n",
      "step 2360, epoch: 42.9091, train loss: 2.7031, grad_norm: 0.48, 1006.82ms/step, 889.93samples/sec\n",
      "step 2370, epoch: 43.0909, train loss: 2.7188, grad_norm: 0.49, 1035.42ms/step, 865.35samples/sec\n",
      "step 2380, epoch: 43.2727, train loss: 2.7812, grad_norm: 0.46, 1065.29ms/step, 841.09samples/sec\n",
      "step 2390, epoch: 43.4545, train loss: 2.7969, grad_norm: 0.47, 1032.24ms/step, 868.02samples/sec\n",
      "step 2400, epoch: 43.6364, train loss: 2.8125, grad_norm: 0.48, 1033.38ms/step, 867.06samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 20.19it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:23<00:00,  8.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2400, eval loss: 2.9969, clipscore: 22.78\n",
      "step 2410, epoch: 43.8182, train loss: 2.7656, grad_norm: 0.55, 9705.35ms/step, 92.32samples/sec\n",
      "step 2420, epoch: 44.0000, train loss: 2.7812, grad_norm: 0.45, 1025.48ms/step, 873.74samples/sec\n",
      "step 2430, epoch: 44.1818, train loss: 2.7500, grad_norm: 0.47, 1061.21ms/step, 844.32samples/sec\n",
      "step 2440, epoch: 44.3636, train loss: 2.7344, grad_norm: 0.46, 1029.53ms/step, 870.30samples/sec\n",
      "step 2450, epoch: 44.5455, train loss: 2.7031, grad_norm: 0.48, 1065.88ms/step, 840.62samples/sec\n",
      "step 2460, epoch: 44.7273, train loss: 2.7656, grad_norm: 0.48, 1029.51ms/step, 870.32samples/sec\n",
      "step 2470, epoch: 44.9091, train loss: 2.8125, grad_norm: 0.49, 1011.59ms/step, 885.74samples/sec\n",
      "step 2480, epoch: 45.0909, train loss: 2.7188, grad_norm: 0.50, 1097.00ms/step, 816.78samples/sec\n",
      "step 2490, epoch: 45.2727, train loss: 2.7031, grad_norm: 0.46, 1062.22ms/step, 843.52samples/sec\n",
      "step 2500, epoch: 45.4545, train loss: 2.7031, grad_norm: 0.49, 1063.69ms/step, 842.35samples/sec\n",
      "step 2510, epoch: 45.6364, train loss: 2.6094, grad_norm: 0.48, 1112.83ms/step, 805.16samples/sec\n",
      "step 2520, epoch: 45.8182, train loss: 2.7500, grad_norm: 0.57, 1043.21ms/step, 858.89samples/sec\n",
      "step 2530, epoch: 46.0000, train loss: 2.7344, grad_norm: 0.47, 1109.22ms/step, 807.77samples/sec\n",
      "step 2540, epoch: 46.1818, train loss: 2.7344, grad_norm: 0.49, 1064.20ms/step, 841.95samples/sec\n",
      "step 2550, epoch: 46.3636, train loss: 2.6719, grad_norm: 0.43, 1066.83ms/step, 839.87samples/sec\n",
      "step 2560, epoch: 46.5455, train loss: 2.6875, grad_norm: 0.47, 1102.34ms/step, 812.82samples/sec\n",
      "step 2570, epoch: 46.7273, train loss: 2.7031, grad_norm: 0.48, 1064.75ms/step, 841.51samples/sec\n",
      "step 2580, epoch: 46.9091, train loss: 2.7812, grad_norm: 0.50, 1085.28ms/step, 825.59samples/sec\n",
      "step 2590, epoch: 47.0909, train loss: 2.7188, grad_norm: 0.45, 1065.56ms/step, 840.87samples/sec\n",
      "step 2600, epoch: 47.2727, train loss: 2.7344, grad_norm: 0.55, 1045.11ms/step, 857.33samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 58.50it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:23<00:00,  8.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2600, eval loss: 2.8484, clipscore: 23.04\n",
      "step 2610, epoch: 47.4545, train loss: 2.6562, grad_norm: 0.52, 9740.29ms/step, 91.99samples/sec\n",
      "step 2620, epoch: 47.6364, train loss: 2.6719, grad_norm: 0.43, 1029.29ms/step, 870.51samples/sec\n",
      "step 2630, epoch: 47.8182, train loss: 2.6875, grad_norm: 0.49, 1028.64ms/step, 871.05samples/sec\n",
      "step 2640, epoch: 48.0000, train loss: 2.7344, grad_norm: 0.50, 1041.91ms/step, 859.95samples/sec\n",
      "step 2650, epoch: 48.1818, train loss: 2.6875, grad_norm: 0.48, 1031.57ms/step, 868.58samples/sec\n",
      "step 2660, epoch: 48.3636, train loss: 2.7344, grad_norm: 0.52, 1067.61ms/step, 839.26samples/sec\n",
      "step 2670, epoch: 48.5455, train loss: 2.6875, grad_norm: 0.50, 1032.99ms/step, 867.39samples/sec\n",
      "step 2680, epoch: 48.7273, train loss: 2.7188, grad_norm: 0.46, 1033.67ms/step, 866.81samples/sec\n",
      "step 2690, epoch: 48.9091, train loss: 2.6719, grad_norm: 0.50, 1047.20ms/step, 855.62samples/sec\n",
      "step 2700, epoch: 49.0909, train loss: 2.7188, grad_norm: 0.45, 1034.15ms/step, 866.41samples/sec\n",
      "step 2710, epoch: 49.2727, train loss: 2.6406, grad_norm: 0.47, 1067.10ms/step, 839.66samples/sec\n",
      "step 2720, epoch: 49.4545, train loss: 2.7031, grad_norm: 0.52, 1049.71ms/step, 853.57samples/sec\n",
      "step 2730, epoch: 49.6364, train loss: 2.7188, grad_norm: 0.48, 1105.25ms/step, 810.67samples/sec\n",
      "step 2740, epoch: 49.8182, train loss: 2.7344, grad_norm: 0.55, 1042.61ms/step, 859.38samples/sec\n",
      "step 2750, epoch: 50.0000, train loss: 2.6719, grad_norm: 0.46, 1015.56ms/step, 882.27samples/sec\n",
      "step 2760, epoch: 50.1818, train loss: 2.6719, grad_norm: 0.47, 1071.12ms/step, 836.51samples/sec\n",
      "step 2770, epoch: 50.3636, train loss: 2.6250, grad_norm: 0.48, 1029.79ms/step, 870.08samples/sec\n",
      "step 2780, epoch: 50.5455, train loss: 2.7188, grad_norm: 0.52, 1032.80ms/step, 867.54samples/sec\n",
      "step 2790, epoch: 50.7273, train loss: 2.7812, grad_norm: 0.52, 1066.64ms/step, 840.02samples/sec\n",
      "step 2800, epoch: 50.9091, train loss: 2.6875, grad_norm: 0.44, 1014.07ms/step, 883.56samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 60.63it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:23<00:00,  8.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2800, eval loss: 2.8734, clipscore: 23.66\n",
      "step 2810, epoch: 51.0909, train loss: 2.5625, grad_norm: 0.48, 9818.20ms/step, 91.26samples/sec\n",
      "step 2820, epoch: 51.2727, train loss: 2.5938, grad_norm: 0.43, 1032.61ms/step, 867.70samples/sec\n",
      "step 2830, epoch: 51.4545, train loss: 2.7031, grad_norm: 0.50, 1027.78ms/step, 871.78samples/sec\n",
      "step 2840, epoch: 51.6364, train loss: 2.6719, grad_norm: 0.49, 1029.94ms/step, 869.95samples/sec\n",
      "step 2850, epoch: 51.8182, train loss: 2.7969, grad_norm: 0.47, 1028.77ms/step, 870.95samples/sec\n",
      "step 2940, epoch: 53.4545, train loss: 2.6406, grad_norm: 0.54, 1063.02ms/step, 842.88samples/sec\n",
      "step 2950, epoch: 53.6364, train loss: 2.6250, grad_norm: 0.48, 1033.15ms/step, 867.25samples/sec\n",
      "step 2960, epoch: 53.8182, train loss: 2.6719, grad_norm: 0.48, 1064.79ms/step, 841.48samples/sec\n",
      "step 2970, epoch: 54.0000, train loss: 2.5938, grad_norm: 0.49, 1012.10ms/step, 885.28samples/sec\n",
      "step 2980, epoch: 54.1818, train loss: 2.6562, grad_norm: 0.52, 1033.93ms/step, 866.60samples/sec\n",
      "step 2990, epoch: 54.3636, train loss: 2.6875, grad_norm: 0.52, 1068.18ms/step, 838.81samples/sec\n",
      "step 3000, epoch: 54.5455, train loss: 2.7188, grad_norm: 0.51, 1030.52ms/step, 869.46samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 60.54it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:24<00:00,  8.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000, eval loss: 3.0500, clipscore: 22.93\n",
      "step 3010, epoch: 54.7273, train loss: 2.5625, grad_norm: 0.47, 9807.15ms/step, 91.36samples/sec\n",
      "step 3020, epoch: 54.9091, train loss: 2.6094, grad_norm: 0.50, 1062.65ms/step, 843.17samples/sec\n",
      "step 3030, epoch: 55.0909, train loss: 2.6719, grad_norm: 0.55, 1010.06ms/step, 887.07samples/sec\n",
      "step 3040, epoch: 55.2727, train loss: 2.6562, grad_norm: 0.49, 1063.04ms/step, 842.87samples/sec\n",
      "step 3050, epoch: 55.4545, train loss: 2.6875, grad_norm: 0.50, 1032.44ms/step, 867.85samples/sec\n",
      "step 3060, epoch: 55.6364, train loss: 2.6562, grad_norm: 0.52, 1028.82ms/step, 870.90samples/sec\n",
      "step 3070, epoch: 55.8182, train loss: 2.6250, grad_norm: 0.59, 1065.85ms/step, 840.64samples/sec\n",
      "step 3080, epoch: 56.0000, train loss: 2.6562, grad_norm: 0.53, 1012.43ms/step, 885.00samples/sec\n",
      "step 3090, epoch: 56.1818, train loss: 2.6094, grad_norm: 0.55, 1067.78ms/step, 839.12samples/sec\n",
      "step 3100, epoch: 56.3636, train loss: 2.6719, grad_norm: 0.51, 1032.61ms/step, 867.70samples/sec\n",
      "step 3110, epoch: 56.5455, train loss: 2.5625, grad_norm: 0.53, 1029.62ms/step, 870.23samples/sec\n",
      "step 3120, epoch: 56.7273, train loss: 2.6719, grad_norm: 0.50, 1064.70ms/step, 841.55samples/sec\n",
      "step 3130, epoch: 56.9091, train loss: 2.6250, grad_norm: 0.54, 1028.67ms/step, 871.03samples/sec\n",
      "step 3140, epoch: 57.0909, train loss: 2.7031, grad_norm: 0.59, 1044.79ms/step, 857.59samples/sec\n",
      "step 3150, epoch: 57.2727, train loss: 2.5781, grad_norm: 0.53, 1033.41ms/step, 867.03samples/sec\n",
      "step 3160, epoch: 57.4545, train loss: 2.5938, grad_norm: 0.54, 1031.69ms/step, 868.48samples/sec\n",
      "step 3170, epoch: 57.6364, train loss: 2.5781, grad_norm: 0.52, 1062.42ms/step, 843.36samples/sec\n",
      "step 3180, epoch: 57.8182, train loss: 2.6094, grad_norm: 0.53, 1028.47ms/step, 871.20samples/sec\n",
      "step 3190, epoch: 58.0000, train loss: 2.6406, grad_norm: 0.60, 1104.78ms/step, 811.02samples/sec\n",
      "step 3200, epoch: 58.1818, train loss: 2.6094, grad_norm: 0.55, 1035.58ms/step, 865.22samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 59.69it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:24<00:00,  8.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3200, eval loss: 2.8891, clipscore: 23.25\n",
      "step 3210, epoch: 58.3636, train loss: 2.5156, grad_norm: 0.62, 9759.93ms/step, 91.80samples/sec\n",
      "step 3220, epoch: 58.5455, train loss: 2.5469, grad_norm: 0.61, 1059.53ms/step, 845.66samples/sec\n",
      "step 3230, epoch: 58.7273, train loss: 2.6719, grad_norm: 0.54, 1029.54ms/step, 870.29samples/sec\n",
      "step 3240, epoch: 58.9091, train loss: 2.5781, grad_norm: 0.51, 1059.94ms/step, 845.33samples/sec\n",
      "step 3250, epoch: 59.0909, train loss: 2.6406, grad_norm: 0.55, 1006.90ms/step, 889.86samples/sec\n",
      "step 3260, epoch: 59.2727, train loss: 2.6250, grad_norm: 0.59, 1029.35ms/step, 870.45samples/sec\n",
      "step 3270, epoch: 59.4545, train loss: 2.5781, grad_norm: 0.60, 1063.36ms/step, 842.61samples/sec\n",
      "step 3280, epoch: 59.6364, train loss: 2.6250, grad_norm: 0.48, 1029.71ms/step, 870.15samples/sec\n",
      "step 3290, epoch: 59.8182, train loss: 2.6250, grad_norm: 0.54, 1027.51ms/step, 872.01samples/sec\n",
      "step 3300, epoch: 60.0000, train loss: 2.6719, grad_norm: 0.50, 1062.59ms/step, 843.22samples/sec\n",
      "step 3310, epoch: 60.1818, train loss: 2.5469, grad_norm: 0.58, 1010.56ms/step, 886.64samples/sec\n",
      "step 3320, epoch: 60.3636, train loss: 2.6250, grad_norm: 0.58, 1068.03ms/step, 838.93samples/sec\n",
      "step 3330, epoch: 60.5455, train loss: 2.6094, grad_norm: 0.58, 1032.42ms/step, 867.86samples/sec\n",
      "step 3340, epoch: 60.7273, train loss: 2.5625, grad_norm: 0.61, 1029.47ms/step, 870.35samples/sec\n",
      "step 3350, epoch: 60.9091, train loss: 2.6250, grad_norm: 0.62, 1061.67ms/step, 843.95samples/sec\n",
      "step 3360, epoch: 61.0909, train loss: 2.6094, grad_norm: 0.62, 1010.72ms/step, 886.50samples/sec\n",
      "step 3370, epoch: 61.2727, train loss: 2.5625, grad_norm: 0.55, 1069.86ms/step, 837.49samples/sec\n",
      "step 3380, epoch: 61.4545, train loss: 2.6250, grad_norm: 0.64, 1031.72ms/step, 868.45samples/sec\n",
      "step 3390, epoch: 61.6364, train loss: 2.6250, grad_norm: 0.54, 1030.91ms/step, 869.14samples/sec\n",
      "step 3400, epoch: 61.8182, train loss: 2.7031, grad_norm: 0.55, 1065.00ms/step, 841.32samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 60.33it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:23<00:00,  8.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3400, eval loss: 3.0734, clipscore: 23.19\n",
      "step 3410, epoch: 62.0000, train loss: 2.5156, grad_norm: 0.54, 9723.10ms/step, 92.15samples/sec\n",
      "step 3420, epoch: 62.1818, train loss: 2.6406, grad_norm: 0.50, 1079.78ms/step, 829.80samples/sec\n",
      "step 3430, epoch: 62.3636, train loss: 2.5781, grad_norm: 0.58, 1057.96ms/step, 846.91samples/sec\n",
      "step 3440, epoch: 62.5455, train loss: 2.6094, grad_norm: 0.57, 1058.95ms/step, 846.12samples/sec\n",
      "step 3450, epoch: 62.7273, train loss: 2.6094, grad_norm: 0.54, 1101.51ms/step, 813.43samples/sec\n",
      "step 3460, epoch: 62.9091, train loss: 2.5938, grad_norm: 0.52, 1058.16ms/step, 846.75samples/sec\n",
      "step 3470, epoch: 63.0909, train loss: 2.5625, grad_norm: 0.59, 1101.50ms/step, 813.44samples/sec\n",
      "step 3480, epoch: 63.2727, train loss: 2.5938, grad_norm: 0.60, 1038.38ms/step, 862.89samples/sec\n",
      "step 3490, epoch: 63.4545, train loss: 2.5625, grad_norm: 0.54, 1060.13ms/step, 845.18samples/sec\n",
      "step 3500, epoch: 63.6364, train loss: 2.5625, grad_norm: 0.61, 1104.24ms/step, 811.42samples/sec\n",
      "step 3510, epoch: 63.8182, train loss: 2.5000, grad_norm: 0.59, 1059.01ms/step, 846.07samples/sec\n",
      "step 3520, epoch: 64.0000, train loss: 2.6094, grad_norm: 0.58, 1059.94ms/step, 845.33samples/sec\n",
      "step 3530, epoch: 64.1818, train loss: 2.6094, grad_norm: 0.66, 1078.69ms/step, 830.64samples/sec\n",
      "step 3540, epoch: 64.3636, train loss: 2.6406, grad_norm: 0.60, 1061.34ms/step, 844.22samples/sec\n",
      "step 3550, epoch: 64.5455, train loss: 2.5469, grad_norm: 0.59, 1106.68ms/step, 809.63samples/sec\n",
      "step 3560, epoch: 64.7273, train loss: 2.5781, grad_norm: 0.59, 1058.06ms/step, 846.84samples/sec\n",
      "step 3570, epoch: 64.9091, train loss: 2.5625, grad_norm: 0.51, 1058.45ms/step, 846.52samples/sec\n",
      "step 3580, epoch: 65.0909, train loss: 2.6094, grad_norm: 0.62, 1097.95ms/step, 816.07samples/sec\n",
      "step 3590, epoch: 65.2727, train loss: 2.5625, grad_norm: 0.50, 1042.60ms/step, 859.39samples/sec\n",
      "step 3600, epoch: 65.4545, train loss: 2.5938, grad_norm: 0.59, 1100.84ms/step, 813.93samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 58.16it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:21<00:00,  8.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3600, eval loss: 2.9516, clipscore: 23.24\n",
      "step 3610, epoch: 65.6364, train loss: 2.4688, grad_norm: 0.55, 9498.78ms/step, 94.33samples/sec\n",
      "step 3620, epoch: 65.8182, train loss: 2.5156, grad_norm: 0.54, 1026.78ms/step, 872.63samples/sec\n",
      "step 3630, epoch: 66.0000, train loss: 2.5938, grad_norm: 0.60, 1063.12ms/step, 842.80samples/sec\n",
      "step 3640, epoch: 66.1818, train loss: 2.5625, grad_norm: 0.57, 1006.46ms/step, 890.25samples/sec\n",
      "step 3650, epoch: 66.3636, train loss: 2.5938, grad_norm: 0.63, 1063.71ms/step, 842.34samples/sec\n",
      "step 3660, epoch: 66.5455, train loss: 2.5625, grad_norm: 0.61, 1027.87ms/step, 871.71samples/sec\n",
      "step 3670, epoch: 66.7273, train loss: 2.5000, grad_norm: 0.53, 1026.29ms/step, 873.05samples/sec\n",
      "step 3680, epoch: 66.9091, train loss: 2.5781, grad_norm: 0.62, 1063.67ms/step, 842.37samples/sec\n",
      "step 3690, epoch: 67.0909, train loss: 2.5469, grad_norm: 0.54, 1028.56ms/step, 871.12samples/sec\n",
      "step 3700, epoch: 67.2727, train loss: 2.5469, grad_norm: 0.56, 1044.00ms/step, 858.24samples/sec\n",
      "step 3710, epoch: 67.4545, train loss: 2.5000, grad_norm: 0.55, 1027.72ms/step, 871.83samples/sec\n",
      "step 3720, epoch: 67.6364, train loss: 2.5781, grad_norm: 0.50, 1031.36ms/step, 868.76samples/sec\n",
      "step 3730, epoch: 67.8182, train loss: 2.5156, grad_norm: 0.59, 1060.60ms/step, 844.81samples/sec\n",
      "step 3740, epoch: 68.0000, train loss: 2.5625, grad_norm: 0.66, 1027.94ms/step, 871.64samples/sec\n",
      "step 3750, epoch: 68.1818, train loss: 2.5938, grad_norm: 0.63, 1063.40ms/step, 842.58samples/sec\n",
      "step 3760, epoch: 68.3636, train loss: 2.5156, grad_norm: 0.55, 1006.20ms/step, 890.48samples/sec\n",
      "step 3770, epoch: 68.5455, train loss: 2.5469, grad_norm: 0.54, 1028.52ms/step, 871.15samples/sec\n",
      "step 3780, epoch: 68.7273, train loss: 2.5781, grad_norm: 0.64, 1062.94ms/step, 842.95samples/sec\n",
      "step 3790, epoch: 68.9091, train loss: 2.6094, grad_norm: 0.63, 1029.11ms/step, 870.66samples/sec\n",
      "step 3800, epoch: 69.0909, train loss: 2.6250, grad_norm: 0.56, 1027.95ms/step, 871.64samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 61.88it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:24<00:00,  8.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3800, eval loss: 3.1250, clipscore: 22.48\n",
      "step 3810, epoch: 69.2727, train loss: 2.6094, grad_norm: 0.59, 9823.44ms/step, 91.21samples/sec\n",
      "step 3820, epoch: 69.4545, train loss: 2.6250, grad_norm: 0.58, 1059.91ms/step, 845.35samples/sec\n",
      "step 3830, epoch: 69.6364, train loss: 2.5781, grad_norm: 0.57, 1081.61ms/step, 828.40samples/sec\n",
      "step 3840, epoch: 69.8182, train loss: 2.5625, grad_norm: 0.59, 1025.92ms/step, 873.36samples/sec\n",
      "step 3850, epoch: 70.0000, train loss: 2.5625, grad_norm: 0.62, 1028.02ms/step, 871.58samples/sec\n",
      "step 3860, epoch: 70.1818, train loss: 2.5781, grad_norm: 0.56, 1058.75ms/step, 846.28samples/sec\n",
      "step 3870, epoch: 70.3636, train loss: 2.6406, grad_norm: 0.54, 1024.24ms/step, 874.80samples/sec\n",
      "step 3880, epoch: 70.5455, train loss: 2.5625, grad_norm: 0.57, 1064.62ms/step, 841.61samples/sec\n",
      "step 3890, epoch: 70.7273, train loss: 2.5469, grad_norm: 0.53, 1030.97ms/step, 869.09samples/sec\n",
      "step 3900, epoch: 70.9091, train loss: 2.5312, grad_norm: 0.54, 1029.12ms/step, 870.65samples/sec\n",
      "step 3910, epoch: 71.0909, train loss: 2.4531, grad_norm: 0.54, 1060.80ms/step, 844.64samples/sec\n",
      "step 3920, epoch: 71.2727, train loss: 2.5625, grad_norm: 0.58, 1017.74ms/step, 880.38samples/sec\n",
      "step 3930, epoch: 71.4545, train loss: 2.5625, grad_norm: 0.55, 1069.32ms/step, 837.92samples/sec\n",
      "step 3940, epoch: 71.6364, train loss: 2.5625, grad_norm: 0.55, 1051.29ms/step, 852.28samples/sec\n",
      "step 3950, epoch: 71.8182, train loss: 2.5156, grad_norm: 0.54, 1029.46ms/step, 870.36samples/sec\n",
      "step 3960, epoch: 72.0000, train loss: 2.5469, grad_norm: 0.58, 1062.02ms/step, 843.67samples/sec\n",
      "step 3970, epoch: 72.1818, train loss: 2.5312, grad_norm: 0.51, 1032.12ms/step, 868.12samples/sec\n",
      "step 3980, epoch: 72.3636, train loss: 2.6094, grad_norm: 0.66, 1043.09ms/step, 858.99samples/sec\n",
      "step 3990, epoch: 72.5455, train loss: 2.5781, grad_norm: 0.62, 1029.81ms/step, 870.06samples/sec\n",
      "step 4000, epoch: 72.7273, train loss: 2.5781, grad_norm: 0.67, 1029.46ms/step, 870.36samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 59.62it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:23<00:00,  8.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000, eval loss: 2.9547, clipscore: 23.23\n",
      "step 4010, epoch: 72.9091, train loss: 2.4688, grad_norm: 0.53, 9736.25ms/step, 92.03samples/sec\n",
      "step 4020, epoch: 73.0909, train loss: 2.5156, grad_norm: 0.55, 1039.46ms/step, 861.99samples/sec\n",
      "step 4030, epoch: 73.2727, train loss: 2.5469, grad_norm: 0.54, 1110.11ms/step, 807.13samples/sec\n",
      "step 4130, epoch: 75.0909, train loss: 2.5469, grad_norm: 0.57, 1029.49ms/step, 870.33samples/sec\n",
      "step 4140, epoch: 75.2727, train loss: 2.5469, grad_norm: 0.66, 1062.36ms/step, 843.40samples/sec\n",
      "step 4150, epoch: 75.4545, train loss: 2.4844, grad_norm: 0.56, 1013.82ms/step, 883.79samples/sec\n",
      "step 4160, epoch: 75.6364, train loss: 2.5000, grad_norm: 0.54, 1085.86ms/step, 825.15samples/sec\n",
      "step 4170, epoch: 75.8182, train loss: 2.4375, grad_norm: 0.58, 1060.38ms/step, 844.98samples/sec\n",
      "step 4180, epoch: 76.0000, train loss: 2.5312, grad_norm: 0.67, 1039.16ms/step, 862.24samples/sec\n",
      "step 4190, epoch: 76.1818, train loss: 2.5781, grad_norm: 0.59, 1061.76ms/step, 843.88samples/sec\n",
      "step 4200, epoch: 76.3636, train loss: 2.5312, grad_norm: 0.54, 1031.28ms/step, 868.82samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 59.00it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:23<00:00,  8.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4200, eval loss: 2.9719, clipscore: 23.71\n",
      "step 4210, epoch: 76.5455, train loss: 2.3750, grad_norm: 0.58, 9795.07ms/step, 91.47samples/sec\n",
      "step 4220, epoch: 76.7273, train loss: 2.4375, grad_norm: 0.52, 1050.62ms/step, 852.83samples/sec\n",
      "step 4230, epoch: 76.9091, train loss: 2.5312, grad_norm: 0.56, 1053.14ms/step, 850.79samples/sec\n",
      "step 4240, epoch: 77.0909, train loss: 2.5312, grad_norm: 0.56, 1100.19ms/step, 814.41samples/sec\n",
      "step 4250, epoch: 77.2727, train loss: 2.6250, grad_norm: 0.57, 1056.43ms/step, 848.14samples/sec\n",
      "step 4260, epoch: 77.4545, train loss: 2.5156, grad_norm: 0.53, 1082.10ms/step, 828.02samples/sec\n",
      "step 4270, epoch: 77.6364, train loss: 2.5156, grad_norm: 0.57, 1061.93ms/step, 843.74samples/sec\n",
      "step 4280, epoch: 77.8182, train loss: 2.5156, grad_norm: 0.54, 1057.90ms/step, 846.96samples/sec\n",
      "step 4290, epoch: 78.0000, train loss: 2.5312, grad_norm: 0.53, 1066.67ms/step, 840.00samples/sec\n",
      "step 4300, epoch: 78.1818, train loss: 2.5781, grad_norm: 0.55, 1055.58ms/step, 848.82samples/sec\n",
      "step 4310, epoch: 78.3636, train loss: 2.5781, grad_norm: 0.54, 1058.84ms/step, 846.21samples/sec\n",
      "step 4320, epoch: 78.5455, train loss: 2.5312, grad_norm: 0.64, 1063.78ms/step, 842.28samples/sec\n",
      "step 4330, epoch: 78.7273, train loss: 2.4844, grad_norm: 0.63, 1056.87ms/step, 847.79samples/sec\n",
      "step 4340, epoch: 78.9091, train loss: 2.4688, grad_norm: 0.60, 1065.98ms/step, 840.54samples/sec\n",
      "step 4350, epoch: 79.0909, train loss: 2.4844, grad_norm: 0.54, 1029.96ms/step, 869.94samples/sec\n",
      "step 4360, epoch: 79.2727, train loss: 2.5156, grad_norm: 0.65, 1030.75ms/step, 869.27samples/sec\n",
      "step 4370, epoch: 79.4545, train loss: 2.4062, grad_norm: 0.60, 1038.50ms/step, 862.78samples/sec\n",
      "step 4380, epoch: 79.6364, train loss: 2.4844, grad_norm: 0.57, 1028.23ms/step, 871.40samples/sec\n",
      "step 4390, epoch: 79.8182, train loss: 2.4688, grad_norm: 0.56, 1059.27ms/step, 845.87samples/sec\n",
      "step 4400, epoch: 80.0000, train loss: 2.5156, grad_norm: 0.62, 1025.49ms/step, 873.73samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 59.81it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:23<00:00,  8.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4400, eval loss: 3.1844, clipscore: 23.54\n",
      "step 4410, epoch: 80.1818, train loss: 2.3750, grad_norm: 0.59, 9744.73ms/step, 91.95samples/sec\n",
      "step 4420, epoch: 80.3636, train loss: 2.4062, grad_norm: 0.56, 1087.10ms/step, 824.21samples/sec\n",
      "step 4430, epoch: 80.5455, train loss: 2.5000, grad_norm: 0.63, 1034.11ms/step, 866.45samples/sec\n",
      "step 4440, epoch: 80.7273, train loss: 2.4688, grad_norm: 0.58, 1102.99ms/step, 812.34samples/sec\n",
      "step 4450, epoch: 80.9091, train loss: 2.4688, grad_norm: 0.64, 1055.81ms/step, 848.64samples/sec\n",
      "step 4460, epoch: 81.0909, train loss: 2.4531, grad_norm: 0.64, 1057.11ms/step, 847.59samples/sec\n",
      "step 4470, epoch: 81.2727, train loss: 2.4531, grad_norm: 0.60, 1098.51ms/step, 815.65samples/sec\n",
      "step 4480, epoch: 81.4545, train loss: 2.4688, grad_norm: 0.62, 1035.96ms/step, 864.90samples/sec\n",
      "step 4490, epoch: 81.6364, train loss: 2.4219, grad_norm: 0.58, 1101.38ms/step, 813.53samples/sec\n",
      "step 4500, epoch: 81.8182, train loss: 2.4844, grad_norm: 0.58, 1059.56ms/step, 845.63samples/sec\n",
      "step 4510, epoch: 82.0000, train loss: 2.3594, grad_norm: 0.58, 1059.22ms/step, 845.91samples/sec\n",
      "step 4520, epoch: 82.1818, train loss: 2.4688, grad_norm: 0.67, 1100.80ms/step, 813.95samples/sec\n",
      "step 4530, epoch: 82.3636, train loss: 2.4062, grad_norm: 0.61, 1057.14ms/step, 847.57samples/sec\n",
      "step 4540, epoch: 82.5455, train loss: 2.5000, grad_norm: 0.62, 1081.83ms/step, 828.23samples/sec\n",
      "step 4550, epoch: 82.7273, train loss: 2.4062, grad_norm: 0.61, 1059.77ms/step, 845.47samples/sec\n",
      "step 4560, epoch: 82.9091, train loss: 2.4062, grad_norm: 0.65, 1064.40ms/step, 841.78samples/sec\n",
      "step 4570, epoch: 83.0909, train loss: 2.4062, grad_norm: 0.57, 1071.09ms/step, 836.53samples/sec\n",
      "step 4580, epoch: 83.2727, train loss: 2.4062, grad_norm: 0.62, 1061.70ms/step, 843.93samples/sec\n",
      "step 4590, epoch: 83.4545, train loss: 2.4375, grad_norm: 0.62, 1057.96ms/step, 846.92samples/sec\n",
      "step 4600, epoch: 83.6364, train loss: 2.4062, grad_norm: 0.61, 1078.18ms/step, 831.03samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 60.00it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:23<00:00,  8.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4600, eval loss: 3.0375, clipscore: 23.41\n",
      "step 4610, epoch: 83.8182, train loss: 2.3281, grad_norm: 0.70, 9686.75ms/step, 92.50samples/sec\n",
      "step 4620, epoch: 84.0000, train loss: 2.3594, grad_norm: 0.68, 1088.39ms/step, 823.23samples/sec\n",
      "step 4630, epoch: 84.1818, train loss: 2.4688, grad_norm: 0.62, 1026.74ms/step, 872.67samples/sec\n",
      "step 4640, epoch: 84.3636, train loss: 2.4062, grad_norm: 0.59, 1061.44ms/step, 844.13samples/sec\n",
      "step 4650, epoch: 84.5455, train loss: 2.4375, grad_norm: 0.63, 1005.96ms/step, 890.69samples/sec\n",
      "step 4660, epoch: 84.7273, train loss: 2.4219, grad_norm: 0.63, 1029.76ms/step, 870.11samples/sec\n",
      "step 4670, epoch: 84.9091, train loss: 2.3750, grad_norm: 0.72, 1061.77ms/step, 843.87samples/sec\n",
      "step 4680, epoch: 85.0909, train loss: 2.4219, grad_norm: 0.64, 1029.17ms/step, 870.61samples/sec\n",
      "step 4690, epoch: 85.2727, train loss: 2.4531, grad_norm: 0.61, 1025.60ms/step, 873.64samples/sec\n",
      "step 4700, epoch: 85.4545, train loss: 2.5000, grad_norm: 0.61, 1060.67ms/step, 844.75samples/sec\n",
      "step 4710, epoch: 85.6364, train loss: 2.3438, grad_norm: 0.62, 1007.06ms/step, 889.72samples/sec\n",
      "step 4720, epoch: 85.8182, train loss: 2.4219, grad_norm: 0.67, 1062.68ms/step, 843.15samples/sec\n",
      "step 4730, epoch: 86.0000, train loss: 2.3750, grad_norm: 0.70, 1026.33ms/step, 873.02samples/sec\n",
      "step 4740, epoch: 86.1818, train loss: 2.3750, grad_norm: 0.71, 1027.58ms/step, 871.95samples/sec\n",
      "step 4750, epoch: 86.3636, train loss: 2.4062, grad_norm: 0.63, 1060.31ms/step, 845.04samples/sec\n",
      "step 4760, epoch: 86.5455, train loss: 2.3906, grad_norm: 0.61, 1008.79ms/step, 888.20samples/sec\n",
      "step 4770, epoch: 86.7273, train loss: 2.3750, grad_norm: 0.80, 1060.75ms/step, 844.69samples/sec\n",
      "step 4780, epoch: 86.9091, train loss: 2.4062, grad_norm: 0.69, 1028.61ms/step, 871.08samples/sec\n",
      "step 4790, epoch: 87.0909, train loss: 2.4062, grad_norm: 0.61, 1027.60ms/step, 871.93samples/sec\n",
      "step 4800, epoch: 87.2727, train loss: 2.5000, grad_norm: 0.64, 1064.48ms/step, 841.72samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 60.50it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:24<00:00,  8.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4800, eval loss: 3.2422, clipscore: 23.51\n",
      "step 4810, epoch: 87.4545, train loss: 2.3125, grad_norm: 0.66, 9875.58ms/step, 90.73samples/sec\n",
      "step 4820, epoch: 87.6364, train loss: 2.4531, grad_norm: 0.64, 1037.81ms/step, 863.35samples/sec\n",
      "step 4830, epoch: 87.8182, train loss: 2.3750, grad_norm: 0.61, 1026.32ms/step, 873.02samples/sec\n",
      "step 4840, epoch: 88.0000, train loss: 2.3906, grad_norm: 0.65, 1023.93ms/step, 875.06samples/sec\n",
      "step 4850, epoch: 88.1818, train loss: 2.3750, grad_norm: 0.59, 1058.32ms/step, 846.62samples/sec\n",
      "step 4860, epoch: 88.3636, train loss: 2.4062, grad_norm: 0.67, 1026.34ms/step, 873.00samples/sec\n",
      "step 4870, epoch: 88.5455, train loss: 2.3750, grad_norm: 0.60, 1061.94ms/step, 843.73samples/sec\n",
      "step 4880, epoch: 88.7273, train loss: 2.4062, grad_norm: 0.66, 1004.62ms/step, 891.88samples/sec\n",
      "step 4890, epoch: 88.9091, train loss: 2.3594, grad_norm: 0.61, 1026.88ms/step, 872.55samples/sec\n",
      "step 4900, epoch: 89.0909, train loss: 2.3750, grad_norm: 0.62, 1059.80ms/step, 845.44samples/sec\n",
      "step 4910, epoch: 89.2727, train loss: 2.3281, grad_norm: 0.73, 1026.00ms/step, 873.30samples/sec\n",
      "step 4920, epoch: 89.4545, train loss: 2.4375, grad_norm: 0.71, 1061.10ms/step, 844.40samples/sec\n",
      "step 4930, epoch: 89.6364, train loss: 2.4219, grad_norm: 0.62, 1010.00ms/step, 887.13samples/sec\n",
      "step 4940, epoch: 89.8182, train loss: 2.4531, grad_norm: 0.73, 1025.48ms/step, 873.74samples/sec\n",
      "step 4950, epoch: 90.0000, train loss: 2.3438, grad_norm: 0.60, 1065.73ms/step, 840.74samples/sec\n",
      "step 4960, epoch: 90.1818, train loss: 2.3750, grad_norm: 0.60, 1027.69ms/step, 871.86samples/sec\n",
      "step 4970, epoch: 90.3636, train loss: 2.3594, grad_norm: 0.64, 1027.28ms/step, 872.21samples/sec\n",
      "step 4980, epoch: 90.5455, train loss: 2.4062, grad_norm: 0.70, 1060.74ms/step, 844.69samples/sec\n",
      "step 4990, epoch: 90.7273, train loss: 2.4062, grad_norm: 0.66, 1007.63ms/step, 889.22samples/sec\n",
      "step 5000, epoch: 90.9091, train loss: 2.4062, grad_norm: 0.62, 1064.70ms/step, 841.55samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 59.46it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:24<00:00,  8.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5000, eval loss: 3.0688, clipscore: 23.24\n",
      "step 5010, epoch: 91.0909, train loss: 2.2969, grad_norm: 0.63, 9876.78ms/step, 90.72samples/sec\n",
      "step 5020, epoch: 91.2727, train loss: 2.3438, grad_norm: 0.62, 1023.37ms/step, 875.54samples/sec\n",
      "step 5030, epoch: 91.4545, train loss: 2.4219, grad_norm: 0.68, 1058.35ms/step, 846.60samples/sec\n",
      "step 5040, epoch: 91.6364, train loss: 2.3750, grad_norm: 0.66, 1006.60ms/step, 890.13samples/sec\n",
      "step 5050, epoch: 91.8182, train loss: 2.4062, grad_norm: 0.64, 1062.32ms/step, 843.44samples/sec\n",
      "step 5060, epoch: 92.0000, train loss: 2.3906, grad_norm: 0.60, 1053.26ms/step, 850.69samples/sec\n",
      "step 5070, epoch: 92.1818, train loss: 2.3281, grad_norm: 0.61, 1060.74ms/step, 844.69samples/sec\n",
      "step 5080, epoch: 92.3636, train loss: 2.4062, grad_norm: 0.66, 1062.18ms/step, 843.55samples/sec\n",
      "step 5090, epoch: 92.5455, train loss: 2.3750, grad_norm: 0.63, 1027.46ms/step, 872.06samples/sec\n",
      "step 5100, epoch: 92.7273, train loss: 2.3438, grad_norm: 0.60, 1040.39ms/step, 861.21samples/sec\n",
      "step 5110, epoch: 92.9091, train loss: 2.3125, grad_norm: 0.60, 1028.73ms/step, 870.98samples/sec\n",
      "step 5120, epoch: 93.0909, train loss: 2.3906, grad_norm: 0.68, 1027.05ms/step, 872.40samples/sec\n",
      "step 5130, epoch: 93.2727, train loss: 2.3281, grad_norm: 0.64, 1060.30ms/step, 845.04samples/sec\n",
      "step 5140, epoch: 93.4545, train loss: 2.3906, grad_norm: 0.71, 1028.69ms/step, 871.01samples/sec\n",
      "step 5150, epoch: 93.6364, train loss: 2.3906, grad_norm: 0.54, 1060.87ms/step, 844.59samples/sec\n",
      "step 5160, epoch: 93.8182, train loss: 2.3125, grad_norm: 0.62, 1006.54ms/step, 890.18samples/sec\n",
      "step 5170, epoch: 94.0000, train loss: 2.3438, grad_norm: 0.59, 1026.21ms/step, 873.12samples/sec\n",
      "step 5180, epoch: 94.1818, train loss: 2.3750, grad_norm: 0.64, 1060.83ms/step, 844.62samples/sec\n",
      "step 5190, epoch: 94.3636, train loss: 2.3906, grad_norm: 0.62, 1029.09ms/step, 870.67samples/sec\n",
      "step 5200, epoch: 94.5455, train loss: 2.4219, grad_norm: 0.66, 1061.08ms/step, 844.42samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 61.31it/s]\n",
      "eval_images: 100%|██████████████████████████████| 10/10 [01:24<00:00,  8.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5200, eval loss: 3.2828, clipscore: 23.43\n",
      "step 5210, epoch: 94.7273, train loss: 2.4062, grad_norm: 0.72, 9968.25ms/step, 89.89samples/sec\n",
      "step 5220, epoch: 94.9091, train loss: 2.4375, grad_norm: 0.66, 1054.36ms/step, 849.80samples/sec\n",
      "step 5230, epoch: 95.0909, train loss: 2.3906, grad_norm: 0.64, 1100.11ms/step, 814.46samples/sec\n",
      "step 5240, epoch: 95.2727, train loss: 2.3906, grad_norm: 0.70, 1054.12ms/step, 850.00samples/sec\n",
      "step 5250, epoch: 95.4545, train loss: 2.3750, grad_norm: 0.64, 1099.27ms/step, 815.09samples/sec\n",
      "step 5260, epoch: 95.6364, train loss: 2.3750, grad_norm: 0.63, 1057.50ms/step, 847.28samples/sec\n",
      "step 5270, epoch: 95.8182, train loss: 2.4531, grad_norm: 0.62, 1038.00ms/step, 863.20samples/sec\n",
      "step 5280, epoch: 96.0000, train loss: 2.3750, grad_norm: 0.62, 1098.24ms/step, 815.85samples/sec\n",
      "step 5290, epoch: 96.1818, train loss: 2.3594, grad_norm: 0.62, 1059.06ms/step, 846.03samples/sec\n",
      "step 5300, epoch: 96.3636, train loss: 2.3438, grad_norm: 0.57, 1100.48ms/step, 814.19samples/sec\n",
      "step 5310, epoch: 96.5455, train loss: 2.2812, grad_norm: 0.60, 1058.11ms/step, 846.79samples/sec\n",
      "step 5320, epoch: 96.7273, train loss: 2.3594, grad_norm: 0.62, 1037.36ms/step, 863.73samples/sec\n",
      "step 5330, epoch: 96.9091, train loss: 2.3594, grad_norm: 0.60, 1103.27ms/step, 812.13samples/sec\n",
      "step 5340, epoch: 97.0909, train loss: 2.3750, grad_norm: 0.57, 1033.97ms/step, 866.56samples/sec\n",
      "step 5350, epoch: 97.2727, train loss: 2.3281, grad_norm: 0.56, 1026.99ms/step, 872.45samples/sec\n",
      "step 5360, epoch: 97.4545, train loss: 2.3750, grad_norm: 0.62, 1062.18ms/step, 843.55samples/sec\n",
      "step 5370, epoch: 97.6364, train loss: 2.3594, grad_norm: 0.73, 1027.29ms/step, 872.20samples/sec\n",
      "step 5380, epoch: 97.8182, train loss: 2.4688, grad_norm: 0.78, 1041.51ms/step, 860.29samples/sec\n",
      "step 5390, epoch: 98.0000, train loss: 2.4062, grad_norm: 0.68, 1027.64ms/step, 871.90samples/sec\n",
      "step 5400, epoch: 98.1818, train loss: 2.3750, grad_norm: 0.64, 1028.37ms/step, 871.29samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|████████████████████████████████| 10/10 [00:00<00:00, 59.70it/s]\n",
      "eval_images:  80%|████████████████████████▊      | 8/10 [01:07<00:16,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5500, epoch: 100.0000, train loss: 2.3281, grad_norm: 0.60, 1026.71ms/step, 872.69samples/sec\n",
      "step 5510, epoch: 100.1818, train loss: 2.2344, grad_norm: 0.63, 1063.01ms/step, 842.89samples/sec\n",
      "step 5520, epoch: 100.3636, train loss: 2.3281, grad_norm: 0.66, 1028.23ms/step, 871.40samples/sec\n",
      "step 5530, epoch: 100.5455, train loss: 2.3438, grad_norm: 0.66, 1026.66ms/step, 872.73samples/sec\n",
      "step 5540, epoch: 100.7273, train loss: 2.3594, grad_norm: 0.70, 1060.18ms/step, 845.14samples/sec\n",
      "step 5550, epoch: 100.9091, train loss: 2.3281, grad_norm: 0.63, 1010.83ms/step, 886.40samples/sec\n",
      "step 5560, epoch: 101.0909, train loss: 2.3125, grad_norm: 0.64, 1062.99ms/step, 842.91samples/sec\n",
      "step 5570, epoch: 101.2727, train loss: 2.2500, grad_norm: 0.62, 1028.57ms/step, 871.11samples/sec\n",
      "step 5580, epoch: 101.4545, train loss: 2.3281, grad_norm: 0.67, 1026.52ms/step, 872.85samples/sec\n",
      "step 5590, epoch: 101.6364, train loss: 2.3906, grad_norm: 0.65, 1061.20ms/step, 844.33samples/sec\n"
     ]
    }
   ],
   "source": [
    "if log_wandb: \n",
    "    if wandb.run is not None: wandb.finish()\n",
    "    wandb.init(project=\"Hana\", name=f\"Z-{model_size / 1e6:.2f}M_CIFAR10_LR-{lr}_BS-{bs}_TS-{timesteps_training}_my3090\").log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\") or path.endswith(\".json\"))\n",
    "\n",
    "step = 0\n",
    "last_step_time = time.time()\n",
    "\n",
    "for _ in range(epochs):\n",
    "    for labels, latents, prompts_encoded, prompts_atnmask in dataloader:        \n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = scheduler.timesteps[torch.randint(timesteps_training,(latents.shape[0],))].to(device)\n",
    "        latents_noisy = scheduler.scale_noise(latents, timesteps, noise)        \n",
    "        noise_pred = transformer(latents_noisy, prompts_encoded, timesteps, prompts_atnmask).sample\n",
    "    \n",
    "        loss = F.mse_loss(noise_pred, noise - latents)\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()        \n",
    "        \n",
    "        if step > 0 and step % steps_log == 0:\n",
    "            loss_train = loss.item()\n",
    "            step_time = (time.time() - last_step_time) / steps_log * 1000\n",
    "            sample_tp = bs * steps_log / (time.time() - last_step_time)\n",
    "            print(f\"step {step}, epoch: {step / steps_epoch:.4f}, train loss: {loss_train:.4f}, grad_norm: {grad_norm:.2f}, {step_time:.2f}ms/step, {sample_tp:.2f}samples/sec\")\n",
    "            if log_wandb: wandb.log({\"loss_train\": loss_train, \"grad_norm\": grad_norm, \"step_time\": step_time, \"step\": step, \"sample_tp\": sample_tp, \"sample_count\": step * bs, \"epoch\": step / steps_epoch})\n",
    "            last_step_time = time.time()\n",
    "    \n",
    "        if step % steps_eval == 0:\n",
    "            transformer.eval()\n",
    "            loss_eval = eval_loss(data_val)\n",
    "            scheduler.set_timesteps(timesteps_generate)\n",
    "            images_eval = [generate(p) for p in tqdm([cifar10_labels[k] for k in cifar10_labels], \"eval_images\")]\n",
    "            clipscore = eval_clipscore(images_eval)\n",
    "            print(f\"step {step}, eval loss: {loss_eval:.4f}, clipscore: {clipscore:.2f}\")\n",
    "            if log_wandb: wandb.log({\"loss_eval\": loss_eval, \"clipscore\": clipscore, \"images_eval\": wandb.Image(make_grid(images_eval, 2, 5)), \"step\": step, \"sample_count\": step * bs, \"epoch\": step / steps_epoch})\n",
    "            transformer.train()\n",
    "            scheduler.set_timesteps(timesteps_training)\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de6ec19c-1862-4c89-b46d-6206fd4f8d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c182d5d2cdcc431d8a479f9e63992518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/313M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformer.push_to_hub(f\"g-ronimo/hana-alpha11_cifar10_TS-{timesteps_training}_{epochs}e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2429d2e8-546c-431d-a8a7-1391011d104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !runpodctl remove pod $RUNPOD_POD_ID"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
