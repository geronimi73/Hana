{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "610d29a7-90d0-4aea-a535-56f2ee26145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers accelerate datasets diffusers Pillow==9.4.0 wandb torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a076321-f9eb-4f7d-b3fd-a0b0b418d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from local_secrets import hf_token, wandb_key\n",
    "# from huggingface_hub import login\n",
    "# import wandb\n",
    "\n",
    "# login(token=hf_token)\n",
    "# wandb.login(key=wandb_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60395d2d-a5f3-432d-9d29-cc3e6adf8774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn.functional as F, random, wandb, time\n",
    "import torchvision.transforms as T\n",
    "from diffusers import AutoencoderDC, SanaTransformer2DModel\n",
    "from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n",
    "from transformers import AutoModel, AutoTokenizer, set_seed\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import PIL_to_latent, latent_to_PIL, make_grid, encode_prompt, dcae_scalingf, pil_clipscore\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "539a7f32-9159-48ca-9706-5b0566415fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/diffusers/configuration_utils.py:245: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a model, please use <class 'diffusers.models.transformers.sana_transformer.SanaTransformer2DModel'>.load_config(...) followed by <class 'diffusers.models.transformers.sana_transformer.SanaTransformer2DModel'>.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.bfloat16\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "\n",
    "transformer = SanaTransformer2DModel.from_config(\"transformer_Sana-7L-MBERT_config.json\").to(device).to(dtype)\n",
    "text_encoder = AutoModel.from_pretrained(\"answerdotai/ModernBERT-base\", torch_dtype=dtype).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\", torch_dtype=dtype)\n",
    "\n",
    "model = \"Efficient-Large-Model/Sana_600M_1024px_diffusers\"\n",
    "dcae = AutoencoderDC.from_pretrained(model, subfolder=\"vae\", torch_dtype=dtype).to(device)\n",
    "\n",
    "scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(model, subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2239874e-8912-4d43-a18b-bc764091aecc",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6faef1cf-1565-4957-a5ac-7d5a1f792d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([1, 300, 768]), torch.Size([1, 300]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"g-ronimo/MNIST-latents_dc-ae-f32c32-sana-1.0\")\n",
    "labels = list(range(10))\n",
    "labels_encoded={i: encode_prompt(str(i), tokenizer, text_encoder) for i in labels}\n",
    "\n",
    "len(labels_encoded[0]), labels_encoded[0][0].shape, labels_encoded[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f03b3b78-8ddd-43bd-8e0a-8ded2984cef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,\n",
       " tensor(0.4844, device='cuda:0', dtype=torch.bfloat16),\n",
       " torch.Size([2, 32, 8, 8]),\n",
       " torch.Size([2, 300, 768]),\n",
       " torch.Size([2, 300]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate(items):\n",
    "    labels = [i[\"label\"] for i in items]\n",
    "    latents = torch.cat([torch.Tensor(i[\"latent\"]) for i in items]).to(dtype).to(device)\n",
    "    prompts_encoded = torch.cat([labels_encoded[label][0] for label in labels])\n",
    "    prompts_atnmask = torch.cat([labels_encoded[label][1] for label in labels])\n",
    "\n",
    "    return labels, latents, prompts_encoded, prompts_atnmask\n",
    "\n",
    "dataloader = DataLoader(ds[\"train\"], batch_size=2, shuffle=True, generator = torch.manual_seed(seed), collate_fn=collate)\n",
    "labels, latents, prompts_encoded, prompts_atnmask = next(iter(dataloader))\n",
    "len(labels), latents.mean(), latents.shape, prompts_encoded.shape, prompts_atnmask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb384ce1-f799-4cdd-8b0e-82c6422abde6",
   "metadata": {},
   "source": [
    "# Helpers for eval and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77a90e44-c1c3-4d53-967d-8c3158bcf484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.Image.Image image mode=RGB size=256x256>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(prompt, num_timesteps=10, latent_dim=[1, 32, 8, 8], latent_seed=42):\n",
    "    scheduler.set_timesteps(num_timesteps)\n",
    "    prompt_encoded, prompt_atnmask = encode_prompt(prompt, tokenizer, text_encoder)\n",
    "    latents = torch.randn(latent_dim, generator = torch.manual_seed(latent_seed)).to(dtype).to(device)\n",
    "\n",
    "    for t_idx in range(num_timesteps):\n",
    "        t = scheduler.timesteps[t_idx].unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            noise_pred = transformer(latents, encoder_hidden_states=prompt_encoded, timestep=t, encoder_attention_mask=prompt_atnmask, return_dict=False)[0]\n",
    "        latents = scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "    return latent_to_PIL(latents / dcae_scalingf, dcae)\n",
    "\n",
    "[generate(\"0\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "670f601e-5383-4f49-8d6f-a534f42e63ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|██████████| 10/10 [00:00<00:00, 12.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.5625"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_loss(data_val, num_samples=10, num_timesteps=10, batch_size=24):\n",
    "    losses = []\n",
    "    eval_dataloader = iter(DataLoader(data_val, batch_size=batch_size, shuffle=False, collate_fn=collate))\n",
    "\n",
    "    for i in tqdm(range(num_samples), \"eval_loss\"):\n",
    "        label, latent, prompt_encoded, prompt_atnmask = next(eval_dataloader)\n",
    "        noise = torch.randn_like(latent)\n",
    "        timestep = scheduler.timesteps[[random.randint(0, num_timesteps-1) for _ in range(batch_size)]].to(device)\n",
    "        latent_noisy = scheduler.scale_noise(latent, timestep, noise)\n",
    "        with torch.no_grad():\n",
    "            noise_pred = transformer(latent_noisy, encoder_hidden_states = prompt_encoded, encoder_attention_mask = prompt_atnmask, timestep = timestep, return_dict=False)[0]\n",
    "        loss = F.mse_loss(noise_pred, noise - latent)\n",
    "        losses.append(loss.item())\n",
    "    return sum(losses)/len(losses)\n",
    "\n",
    "eval_loss(ds[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5748ec87-d482-49c0-bf22-c1336eb71584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_clipscore: 100%|██████████| 3/3 [00:04<00:00,  1.35s/it]\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22.915660858154297"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_clipscore(seeds=[1,7,42]):\n",
    "    prompts = [f\"handwritten digit {digit}\" for digit in range(10)]\n",
    "    images = [generate(p, latent_seed=seed) for seed in tqdm(seeds, \"eval_clipscore\") for p in prompts]\n",
    "    return pil_clipscore(images, prompts*len(seeds))\n",
    "\n",
    "eval_clipscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d9761-8eaa-4f23-bfad-cc5fa3ce923f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02081c1d-e87c-438c-a54f-81d9704125da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 156.41M\n"
     ]
    }
   ],
   "source": [
    "log_wandb = True\n",
    "\n",
    "lr = 5e-4\n",
    "# bs = 128\n",
    "bs = 256\n",
    "epochs = 3\n",
    "diffuser_timesteps = 10\n",
    "steps_log, steps_eval = 20, 100\n",
    "\n",
    "data_train, data_val = ds[\"train\"], ds[\"test\"]\n",
    "\n",
    "steps_epoch = len(data_train)\n",
    "steps_total = epochs * (steps_epoch // bs)\n",
    "\n",
    "dataloader = DataLoader(data_train, batch_size=bs, shuffle=True, generator = torch.manual_seed(seed), collate_fn=collate)\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=lr)\n",
    "scheduler.set_timesteps(diffuser_timesteps)\n",
    "\n",
    "model_size = sum(p.numel() for p in transformer.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters: {model_size / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e066f-ff72-4744-bf0c-a6fd090b4241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mg-ronimo\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20250125_153627-clwvgpf7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/g-ronimo/Hana/runs/clwvgpf7' target=\"_blank\">Z-156.41M_MNIST_LR-0.0005_BS-256_10-TS_CLIPSCORE_DATAL</a></strong> to <a href='https://wandb.ai/g-ronimo/Hana' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/g-ronimo/Hana' target=\"_blank\">https://wandb.ai/g-ronimo/Hana</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/g-ronimo/Hana/runs/clwvgpf7' target=\"_blank\">https://wandb.ai/g-ronimo/Hana/runs/clwvgpf7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 20, epoch: 0.0853, train loss: 4.3305, grad_norm: 2.38, 852.66ms/step, 300.24samples/sec\n",
      "step 40, epoch: 0.1707, train loss: 2.1742, grad_norm: 1.22, 839.10ms/step, 305.09samples/sec\n",
      "step 60, epoch: 0.2560, train loss: 1.9945, grad_norm: 1.38, 838.31ms/step, 305.37samples/sec\n",
      "step 80, epoch: 0.3413, train loss: 1.8062, grad_norm: 2.31, 858.82ms/step, 298.08samples/sec\n",
      "step 100, epoch: 0.4267, train loss: 1.5805, grad_norm: 2.94, 845.06ms/step, 302.94samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|██████████| 10/10 [00:00<00:00, 16.18it/s]\n",
      "eval_clipscore: 100%|██████████| 3/3 [00:04<00:00,  1.45s/it]\n",
      "images_eval: 100%|██████████| 10/10 [00:01<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100, eval loss: 1.4625, clipscore: 27.35\n",
      "step 120, epoch: 0.5120, train loss: 1.4301, grad_norm: 1.47, 1589.31ms/step, 161.08samples/sec\n",
      "step 140, epoch: 0.5973, train loss: 1.3367, grad_norm: 1.96, 834.96ms/step, 306.60samples/sec\n",
      "step 160, epoch: 0.6827, train loss: 1.2996, grad_norm: 1.20, 841.12ms/step, 304.35samples/sec\n",
      "step 180, epoch: 0.7680, train loss: 1.2402, grad_norm: 1.45, 839.64ms/step, 304.89samples/sec\n",
      "step 200, epoch: 0.8533, train loss: 1.1941, grad_norm: 1.02, 856.41ms/step, 298.92samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|██████████| 10/10 [00:00<00:00, 16.45it/s]\n",
      "eval_clipscore: 100%|██████████| 3/3 [00:04<00:00,  1.42s/it]\n",
      "images_eval: 100%|██████████| 10/10 [00:01<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200, eval loss: 1.1379, clipscore: 26.86\n",
      "step 220, epoch: 0.9387, train loss: 1.1863, grad_norm: 0.87, 1598.79ms/step, 160.12samples/sec\n",
      "step 240, epoch: 1.0240, train loss: 1.1609, grad_norm: 0.84, 805.90ms/step, 317.66samples/sec\n",
      "step 260, epoch: 1.1093, train loss: 1.1410, grad_norm: 0.75, 832.39ms/step, 307.55samples/sec\n",
      "step 280, epoch: 1.1947, train loss: 1.1031, grad_norm: 0.58, 835.87ms/step, 306.27samples/sec\n",
      "step 300, epoch: 1.2800, train loss: 1.0828, grad_norm: 0.64, 836.32ms/step, 306.10samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|██████████| 10/10 [00:00<00:00, 15.89it/s]\n",
      "eval_clipscore: 100%|██████████| 3/3 [00:04<00:00,  1.39s/it]\n",
      "images_eval: 100%|██████████| 10/10 [00:01<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 300, eval loss: 1.0465, clipscore: 27.33\n",
      "step 320, epoch: 1.3653, train loss: 1.0879, grad_norm: 0.75, 1623.53ms/step, 157.68samples/sec\n",
      "step 340, epoch: 1.4507, train loss: 1.0746, grad_norm: 0.55, 833.69ms/step, 307.07samples/sec\n",
      "step 360, epoch: 1.5360, train loss: 1.0906, grad_norm: 0.64, 830.97ms/step, 308.07samples/sec\n",
      "step 380, epoch: 1.6213, train loss: 1.0559, grad_norm: 0.42, 832.45ms/step, 307.53samples/sec\n",
      "step 400, epoch: 1.7067, train loss: 1.0512, grad_norm: 0.61, 832.06ms/step, 307.67samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|██████████| 10/10 [00:00<00:00, 15.62it/s]\n",
      "eval_clipscore: 100%|██████████| 3/3 [00:04<00:00,  1.44s/it]\n",
      "images_eval: 100%|██████████| 10/10 [00:01<00:00,  6.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400, eval loss: 0.9516, clipscore: 27.25\n",
      "step 420, epoch: 1.7920, train loss: 1.0574, grad_norm: 0.78, 1624.57ms/step, 157.58samples/sec\n",
      "step 440, epoch: 1.8773, train loss: 1.0428, grad_norm: 0.48, 826.60ms/step, 309.70samples/sec\n",
      "step 460, epoch: 1.9627, train loss: 1.0523, grad_norm: 0.60, 825.55ms/step, 310.10samples/sec\n",
      "step 480, epoch: 2.0480, train loss: 1.0201, grad_norm: 0.50, 802.15ms/step, 319.14samples/sec\n",
      "step 500, epoch: 2.1333, train loss: 1.0355, grad_norm: 0.41, 830.77ms/step, 308.15samples/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_loss: 100%|██████████| 10/10 [00:00<00:00, 15.93it/s]\n",
      "eval_clipscore: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]\n",
      "images_eval: 100%|██████████| 10/10 [00:01<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500, eval loss: 1.0203, clipscore: 27.75\n",
      "step 520, epoch: 2.2187, train loss: 1.0270, grad_norm: 0.60, 1578.87ms/step, 162.14samples/sec\n",
      "step 540, epoch: 2.3040, train loss: 1.0078, grad_norm: 0.53, 824.60ms/step, 310.45samples/sec\n",
      "step 560, epoch: 2.3893, train loss: 1.0297, grad_norm: 0.54, 832.96ms/step, 307.34samples/sec\n"
     ]
    }
   ],
   "source": [
    "if log_wandb: wandb.init(project=\"Hana\", name=f\"Z-{model_size / 1e6:.2f}M_MNIST_LR-{lr}_BS-{bs}_10-TS_CLIPSCORE_DATAL\").log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\") or path.endswith(\".json\"))\n",
    "\n",
    "t_start, last_step_time = time.time(), time.time()\n",
    "step, losses = 0, []\n",
    "\n",
    "for _ in range(epochs):\n",
    "    for batch in dataloader:        \n",
    "        transformer.train()\n",
    "        labels, latents, prompts_encoded, prompts_atnmask = batch\n",
    "        noise = torch.randn_like(latents)\n",
    "        timesteps = scheduler.timesteps[torch.randint(diffuser_timesteps,(latents.shape[0],))].to(device)\n",
    "        latents_noisy = scheduler.scale_noise(latents, timesteps, noise)\n",
    "        \n",
    "        noise_pred = transformer(\n",
    "            latents_noisy, \n",
    "            encoder_hidden_states = prompts_encoded, \n",
    "            encoder_attention_mask = prompts_atnmask, \n",
    "            timestep = timesteps, \n",
    "            return_dict=False\n",
    "        )[0]\n",
    "    \n",
    "        loss = F.mse_loss(noise_pred, noise - latents)\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        step += 1\n",
    "        sample_count, epoch = step * bs, step * bs / steps_epoch \n",
    "        \n",
    "        if step % steps_log == 0:\n",
    "            loss_train = sum(losses)/len(losses)\n",
    "            step_time = (time.time() - last_step_time) / steps_log * 1000\n",
    "            sample_tp = bs * steps_log / (time.time() - last_step_time)\n",
    "            print(f\"step {step}, epoch: {epoch:.4f}, train loss: {loss_train:.4f}, grad_norm: {grad_norm:.2f}, {step_time:.2f}ms/step, {sample_tp:.2f}samples/sec\")\n",
    "            if log_wandb: wandb.log({\"loss_train\": loss_train, \"grad_norm\": grad_norm, \"step_time\": step_time, \"step\": step, \"epoch\": epoch, \"sample_tp\": sample_tp, \"sample_count\": sample_count})\n",
    "            last_step_time, losses = time.time(), []\n",
    "    \n",
    "        if step % steps_eval == 0:\n",
    "            transformer.eval()\n",
    "            loss_eval, clipscore, images_eval = eval_loss(data_val), eval_clipscore(), make_grid([generate(str(p)) for p in tqdm(range(10), \"images_eval\")], 2, 5)\n",
    "            print(f\"step {step}, eval loss: {loss_eval:.4f}, clipscore: {clipscore:.2f}\")\n",
    "            if not log_wandb: display(images_eval.resize((300,150)))\n",
    "            if log_wandb: wandb.log({\"loss_eval\": loss_eval, \"clipscore\": clipscore, \"images_eval\": wandb.Image(images_eval), \"step\": step, \"epoch\": epoch, \"sample_count\": sample_count})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ec19c-1862-4c89-b46d-6206fd4f8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.push_to_hub(f\"g-ronimo/hana-small_alpha7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2429d2e8-546c-431d-a8a7-1391011d104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!runpodctl remove pod $RUNPOD_POD_ID"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
