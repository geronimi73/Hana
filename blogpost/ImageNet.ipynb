{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a930a50-0ef9-4254-9d87-709a2697cdce",
   "metadata": {},
   "source": [
    "```\n",
    "diffusers                     0.33.0.dev0\n",
    "torch                         2.4.0\n",
    "torchmetrics                  1.6.1\n",
    "torchvision                   0.19.0\n",
    "transformers                  4.50.0.dev0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b00bd44-5cd3-4908-ab0d-80eb3d4dda73",
   "metadata": {},
   "source": [
    "# ImageNet Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb99e443-ad65-4a81-8320-029ec210b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d56bc-8532-481a-a382-7c6a06701c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\n",
    "    \"visual-layer/imagenet-1k-vl-enriched\", \n",
    "    cache_dir=\"~/ssd-2TB/hf_cache\",\n",
    "    num_proc = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dadde4-4607-4582-82db-7a4336c7bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399d3e6-e2fa-4819-b143-8dbff65f28a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "def make_grid(images, rows, cols, height):\n",
    "    # Check if we have enough images to fill the grid\n",
    "    if len(images) > rows * cols:\n",
    "        print(f\"Warning: Only using the first {rows * cols} images out of {len(images)}\")\n",
    "        images = images[:rows * cols]\n",
    "    \n",
    "    # Resize all images to the specified height while maintaining aspect ratio\n",
    "    resized_images = []\n",
    "    for img in images:\n",
    "        aspect_ratio = img.width / img.height\n",
    "        new_width = int(height * aspect_ratio)\n",
    "        resized_images.append(img.resize((new_width, height), Image.LANCZOS))\n",
    "    \n",
    "    # Calculate total width required for each row and the grid height\n",
    "    row_widths = []\n",
    "    grid_height = 0\n",
    "    \n",
    "    for row in range(rows):\n",
    "        start_idx = row * cols\n",
    "        end_idx = min(start_idx + cols, len(resized_images))\n",
    "        if start_idx >= len(resized_images):\n",
    "            break\n",
    "            \n",
    "        # Sum the widths of all images in this row\n",
    "        row_width = sum(img.width for img in resized_images[start_idx:end_idx])\n",
    "        row_widths.append(row_width)\n",
    "        grid_height += height\n",
    "    \n",
    "    # Find the maximum row width to determine grid width\n",
    "    max_row_width = max(row_widths) if row_widths else 0\n",
    "    \n",
    "    # Create a new blank image for the grid\n",
    "    grid_image = Image.new('RGB', (max_row_width, grid_height), color='white')\n",
    "    \n",
    "    # Paste images into the grid with even spacing across each row\n",
    "    img_index = 0\n",
    "    y_offset = 0\n",
    "    \n",
    "    for row in range(rows):\n",
    "        start_idx = row * cols\n",
    "        end_idx = min(start_idx + cols, len(resized_images))\n",
    "        if start_idx >= len(resized_images):\n",
    "            break\n",
    "            \n",
    "        # Get images for this row\n",
    "        row_images = resized_images[start_idx:end_idx]\n",
    "        num_images = len(row_images)\n",
    "        \n",
    "        # Calculate total width of images in this row\n",
    "        total_img_width = sum(img.width for img in row_images)\n",
    "        \n",
    "        # Calculate spacing between images (if more than one image in the row)\n",
    "        if num_images > 1:\n",
    "            spacing = (max_row_width - total_img_width) / (num_images - 1)\n",
    "        else:\n",
    "            # Center a single image in the row\n",
    "            spacing = 0\n",
    "            \n",
    "        # Place images with calculated spacing\n",
    "        x_offset = 0\n",
    "        for i, img in enumerate(row_images):\n",
    "            grid_image.paste(img, (int(x_offset), y_offset))\n",
    "            x_offset += img.width + spacing\n",
    "            \n",
    "        y_offset += height\n",
    "    \n",
    "    return grid_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61196e16-f5c8-498f-8182-f87fe5f2ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = ds[\"train\"][60:100][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab18d95c-986a-4586-86ca-e6c8b8adfb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=make_grid([\n",
    "    i for i in images\n",
    "], rows=5, cols=5, height=256)\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb19bee-dcd0-4bab-b298-9eb2526f6aa4",
   "metadata": {},
   "source": [
    "# Latent dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb4cc1-5bd8-452c-89b7-396f2e3f2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c068a-4045-4fdf-9e2f-b282f65e38f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderDC\n",
    "\n",
    "model_repo = \"Efficient-Large-Model/Sana_600M_1024px_diffusers\"\n",
    "\n",
    "dcae = AutoencoderDC.from_pretrained(\n",
    "    model_repo, \n",
    "    subfolder=\"vae\", \n",
    "    torch_dtype=dtype\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ddd78-5e52-4b0f-a487-7de901f97a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import latent_to_PIL, PIL_to_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e3525-da6b-4b0c-9c77-93483ca1a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "def resize_max_dim(image, dim=256):\n",
    "    return T.Resize(dim, antialias=True)(image)\n",
    "\n",
    "def resize_closest_multiple(image, denom=32):\n",
    "    w, h = image.size\n",
    "    w = round(w / denom) * denom\n",
    "    h = round(h / denom) * denom\n",
    "    return image.resize((w, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a4c0cb-baa0-4bde-8a6f-64922a6043f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_imgs = []\n",
    "\n",
    "for i in tqdm(range(40)):\n",
    "    image = images[i]\n",
    "    image = image.convert('RGB') if image.mode != \"RGB\" else image\n",
    "    image = resize_max_dim(image)\n",
    "    image = resize_closest_multiple(image)\n",
    "    latent = PIL_to_latent(image, dcae)\n",
    "    latent_imgs.append(latent_to_PIL(latent, dcae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96eb640-e069-4a3f-a9bf-d44d671157e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=make_grid([\n",
    "    i for i in latent_imgs\n",
    "], rows=5, cols=5, height=256)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde50fc3-1e64-4e0c-86ad-e8595d13c2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.Resize(256, antialias=True)(images[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d418986-6285-49a7-8594-0b43de42e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_imgs[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7533e-2c2a-4af3-a7b6-dfa5e6e76a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "T.Resize(256, antialias=True)(images[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1505f8c1-d4cf-4125-a8ee-2fe3b381e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_imgs[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e096ba-4f63-4eb5-9262-5c06d52df60d",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee6216a-3459-4306-aef4-bffb6f51ff39",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c7ff7-7c61-46f8-afed-f63a6233c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a057a6-d21a-4422-be08-e997d3d3cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import SanaTransformer2DModel\n",
    "\n",
    "# Load Sana 600M config\n",
    "config = SanaTransformer2DModel.load_config(\n",
    "    \"Efficient-Large-Model/Sana_600M_1024px_diffusers\", \n",
    "    subfolder=\"transformer\"\n",
    ")\n",
    "\n",
    "# Reduce depth\n",
    "config[\"num_layers\"] = 12\n",
    "\n",
    "# Reduce width\n",
    "config[\"num_attention_heads\"] = 12\n",
    "config[\"attention_head_dim\"] = 64\n",
    "config[\"cross_attention_dim\"] = 768\n",
    "config[\"num_cross_attention_heads\"] = 12\n",
    "config[\"cross_attention_head_dim\"] = 64\n",
    "\n",
    "# Adapt to hidden size of SmolLM2\n",
    "config[\"caption_channels\"] = 960\n",
    "\n",
    "transformer = SanaTransformer2DModel.from_config(config).to(dtype).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd6bb8-7de4-4c58-9ebe-39b4a0046ebe",
   "metadata": {},
   "source": [
    "## Load text encoder and DCAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b9fa2-2d61-4daf-a40b-7559b696b146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from diffusers import AutoencoderDC, SanaTransformer2DModel\n",
    "\n",
    "te_repo = \"HuggingFaceTB/SmolLM2-360M\"\n",
    "sana_repo = \"Efficient-Large-Model/Sana_600M_1024px_diffusers\"\n",
    "\n",
    "# Load the text encoder and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(te_repo, torch_dtype=dtype)\n",
    "# Pad token is not set by default, use eos\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "text_encoder = AutoModel.from_pretrained(te_repo, torch_dtype=dtype).to(device)\n",
    "\n",
    "# Load Deep Compression AutoEncoder\n",
    "dcae = AutoencoderDC.from_pretrained(sana_repo, subfolder=\"vae\", torch_dtype=dtype).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b01dec-d819-48b6-8166-477d97bbeb74",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b241e7e-ec80-44df-9f49-82286a47db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_IN1k256px_AR\n",
    "\n",
    "# load g-ronimo/IN1k256-AR-buckets-bfl16latents_dc-ae-f32c32-sana-1.0\n",
    "# Drop 10% of labels for CFG\n",
    "dataloader_train, dataloader_eval = load_IN1k256px_AR(\n",
    "  batch_size=256, label_dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102a47c7-35a6-487c-9518-b5068a05364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from utils import encode_prompt, add_random_noise\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=0.0005)\n",
    "\n",
    "step = 0 \n",
    "epochs = 100\n",
    "\n",
    "for e in range(epochs):\n",
    "    for labels, latents in dataloader_train:\n",
    "        step += 1\n",
    "        epoch = step/len(dataloader_train)\n",
    "\n",
    "        # Encode prompts\n",
    "        prompts_emb, prompts_atnmask = encode_prompt(labels, max_length=50, tokenizer=tokenizer, text_encoder=text_encoder)\n",
    "\n",
    "        # Scale latent and add random amount of noise\n",
    "        latents = latents.to(dtype).to(device)\n",
    "        latents *=  dcae.config[\"scaling_factor\"]\n",
    "        latents_noisy, timestep, noise = add_random_noise(latents)\n",
    "        \n",
    "        # Get a noise prediction out of the model\n",
    "        noise_pred = transformer(\n",
    "            hidden_states = latents_noisy.to(dtype), \n",
    "            encoder_hidden_states = prompts_emb, \n",
    "            encoder_attention_mask = prompts_atnmask,\n",
    "            timestep = timestep, \n",
    "        ).sample\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate gradients\n",
    "        loss = F.mse_loss(noise_pred, noise - latents)\n",
    "        loss.backward()    \n",
    "\n",
    "        # Clip gradients\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    \n",
    "        if step % 10 == 0:\n",
    "            print(f\"step {step} epoch {epoch:.2f} loss: {loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
