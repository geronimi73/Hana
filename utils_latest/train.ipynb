{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d29a7-90d0-4aea-a535-56f2ee26145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U transformers accelerate datasets git+https://github.com/huggingface/diffusers Pillow==9.4.0 torchmetrics wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a076321-f9eb-4f7d-b3fd-a0b0b418d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from local_secrets import hf_token, wandb_key\n",
    "# from huggingface_hub import login\n",
    "# import wandb\n",
    "\n",
    "# login(token=hf_token)\n",
    "# wandb.login(key=wandb_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60395d2d-a5f3-432d-9d29-cc3e6adf8774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn.functional as F, random, wandb, time\n",
    "import torchvision.transforms as T\n",
    "from torchvision import transforms\n",
    "from diffusers import AutoencoderDC, SanaTransformer2DModel\n",
    "from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n",
    "from transformers import AutoModel, AutoTokenizer, set_seed\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "from utils import latent_to_PIL, make_grid, encode_prompt, dcae_scalingf, pil_clipscore, cifar10_labels, free_memory, mnist_labels\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ccd825-37fc-4668-a867-2ab9e52c6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(text_encoder, transformer_config, ae, dtype, device):\n",
    "    transformer = SanaTransformer2DModel.from_config(transformer_config).to(device).to(dtype)\n",
    "    te = AutoModel.from_pretrained(text_encoder, torch_dtype=dtype).to(device)\n",
    "    tok = AutoTokenizer.from_pretrained(text_encoder, torch_dtype=dtype)\n",
    "    dcae = AutoencoderDC.from_pretrained(ae, subfolder=\"vae\", torch_dtype=dtype).to(device)\n",
    "\n",
    "    print(f\"Transformer parameters: {sum(p.numel() for p in transformer.parameters()) / 1e6:.2f}M\")\n",
    "    print(f\"DCAE parameters: {sum(p.numel() for p in dcae.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "    return transformer, te, tok, dcae\n",
    "\n",
    "def load_data(repo_name, col_latent = \"latent\", col_label = \"label\"):\n",
    "    print(f\"loading dataset {repo_name}\")\n",
    "    \n",
    "    ds = load_dataset(repo_name)\n",
    "    splits = [k for k in ds]\n",
    "    latent_shape = torch.Tensor(ds[splits[0]][0][col_latent]).shape\n",
    "    features = ds[splits[0]].features\n",
    "\n",
    "    assert len(splits)==2\n",
    "    assert len(latent_shape)==4\n",
    "    assert col_latent in features and col_label in features\n",
    "\n",
    "    for i, split in enumerate(splits): print(f\"split #{i} {split}: {len(ds[split])} samples, features: {[k for k in ds[split].features]}\")\n",
    "    print(f\"latent shape {latent_shape}\")\n",
    "\n",
    "    return ds, splits, latent_shape\n",
    "\n",
    "def collate_(items, labels_encoded, col_latent = \"latent\", col_label = \"label\"):\n",
    "    assert col_latent in items[0] and col_label in items[0]\n",
    "    labels = [i[col_label] for i in items]\n",
    "    latents = torch.cat([torch.Tensor(i[col_latent]) for i in items]).to(dtype).to(device)\n",
    "    prompts_encoded = torch.cat([labels_encoded[label][0] for label in labels])\n",
    "    prompts_atnmask = torch.cat([labels_encoded[label][1] for label in labels])\n",
    "\n",
    "    return labels, latents, prompts_encoded, prompts_atnmask\n",
    "\n",
    "def get_dataloaders(ds, splits, bs):\n",
    "    batch_sizes = [bs, bs//2]    # reduce eval_batch size\n",
    "    dataloaders = []\n",
    "    for i, split in enumerate(splits):\n",
    "        batch_size = batch_sizes[i]\n",
    "        print(f\"Assuming split #{i} \\\"{split}\\\" is {'train' if i==0 else 'test'} split, testing batch size {batch_size}\")\n",
    "        dataloader = DataLoader(ds[split], batch_size=batch_size, shuffle=True, generator = torch.manual_seed(seed), collate_fn=collate)\n",
    "        b = next(iter(dataloader))\n",
    "        for i, col in enumerate(b):\n",
    "            coltype = type(col)\n",
    "            collength = len(col) if coltype==list else col.shape\n",
    "            print(f\" col {i} {coltype.__name__} {collength}\")\n",
    "        dataloaders.append(dataloader)\n",
    "    return dataloaders\n",
    "\n",
    "def get_timesteps(num_steps):\n",
    "    dt = 1.0 / num_steps\n",
    "    timesteps = [int(i/num_steps*1000) for i in range(num_steps, 0, -1)]\n",
    "    return dt, timesteps\n",
    "\n",
    "def generate(prompt, tokenizer, text_encoder, latent_dim=None, num_steps=100, latent_seed=42):\n",
    "    assert latent_dim is not None\n",
    "    dt, timesteps = get_timesteps(num_steps)\n",
    "    prompt_encoded, prompt_atnmask = encode_prompt(str(prompt), tokenizer, text_encoder)\n",
    "    latent = torch.randn(latent_dim, generator = torch.manual_seed(latent_seed)).to(dtype).to(device)\n",
    "    for t in timesteps:\n",
    "        t = torch.Tensor([t]).to(dtype).to(device)\n",
    "        with torch.no_grad():\n",
    "            noise_pred = transformer(latent, encoder_hidden_states=prompt_encoded, timestep=t, encoder_attention_mask=prompt_atnmask, return_dict=False)[0]\n",
    "        latent = latent - dt * noise_pred\n",
    "\n",
    "    return latent_to_PIL(latent / dcae_scalingf, dcae)\n",
    "\n",
    "def eval_clipscore(images, labels):\n",
    "    prompts = [labels[k] for k in labels]\n",
    "    return pil_clipscore(images, prompts)\n",
    "\n",
    "def add_random_noise(latents, timesteps=1000):\n",
    "    noise = torch.randn_like(latents)\n",
    "    t = torch.randint(1, timesteps + 1, (latents.size(0),)).to(device)\n",
    "    tperc = t.view([latents.size(0), *([1] * len(latents.shape[1:]))])/timesteps\n",
    "    latents_noisy = (1 - tperc) * latents + tperc * noise # (1-noise_level) * latent + noise_level * noise\n",
    "\n",
    "    return latents_noisy, noise, t\n",
    "\n",
    "def eval_loss(dataloader_eval, timesteps=1000, testing=False):\n",
    "    losses = []\n",
    "\n",
    "    for batch_num, (labels, latents, prompts_encoded, prompts_atnmask) in tqdm(enumerate(dataloader_eval), \"eval_loss\"):\n",
    "        latents = latents * dcae_scalingf\n",
    "        latents_noisy, noise, t = add_random_noise(latents, timesteps)\n",
    "        with torch.no_grad():\n",
    "            noise_pred = transformer(latents_noisy.to(dtype), prompts_encoded, t, prompts_atnmask).sample\n",
    "    \n",
    "        loss = F.mse_loss(noise_pred, noise - latents)\n",
    "        losses.append(loss.item())  \n",
    "        if testing: break\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c950c8e-6f47-41f0-97e5-5497a743fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.bfloat16\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "log_wandb = True\n",
    "lr = 5e-4\n",
    "bs = 256\n",
    "epochs = 1000\n",
    "timesteps_training = 1000\n",
    "steps_log, steps_eval = 10, 100\n",
    "# steps_log, steps_eval = 10, 20\n",
    "wandb_project = \"Hana-trainer-tests\"\n",
    "\n",
    "transformer, text_encoder, tokenizer, dcae = load_models(\n",
    "    text_encoder = \"answerdotai/ModernBERT-base\",\n",
    "    transformer_config = \"transformer_Sana-7L-MBERT_config.json\",\n",
    "    # transformer_config = \"transformer_Sana-xsmall.json\",\n",
    "    ae = \"Efficient-Large-Model/Sana_600M_1024px_diffusers\",\n",
    "    dtype = dtype,\n",
    "    device = device\n",
    ")\n",
    "\n",
    "ds, ds_splits, latent_shape = load_data(\"g-ronimo/MNIST-latents_dc-ae-f32c32-sana-1.0\")\n",
    "labels = mnist_labels\n",
    "eval_labels = labels\n",
    "collate = partial(collate_, labels_encoded = {k: encode_prompt(str(k), tokenizer, text_encoder) for k in labels})\n",
    "dataloader_train, dataloader_eval = get_dataloaders(ds, ds_splits, bs)\n",
    "\n",
    "print(\"Testing generate\")\n",
    "display(generate(\"horse\", tokenizer, text_encoder, num_steps=10, latent_dim=latent_shape).resize((128,128)))\n",
    "\n",
    "print(\"Testing eval loss\")\n",
    "print(eval_loss(dataloader_eval, testing=True))\n",
    "\n",
    "print(\"Testing eval images and clip score\")\n",
    "images = [\n",
    "    generate(p, tokenizer, text_encoder, latent_dim=latent_shape, num_steps=10) \n",
    "    for p in tqdm([labels[k] for k in labels], \"eval_images\")\n",
    "]\n",
    "print(eval_clipscore(images, labels))\n",
    "\n",
    "steps_epoch = len(dataloader_train)\n",
    "print(f\"steps per epoch: {steps_epoch}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=lr)\n",
    "\n",
    "model_size = sum(p.numel() for p in transformer.parameters())\n",
    "wandb_run = f\"{model_size / 1e6:.2f}M_MNIST_LR-{lr}_BS-{bs}_TS-{timesteps_training}_my3090\"\n",
    "\n",
    "del labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d9761-8eaa-4f23-bfad-cc5fa3ce923f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815d77d-33fd-40f2-aba1-08f28d8adce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if log_wandb: \n",
    "    if wandb.run is not None: wandb.finish()\n",
    "    wandb.init(project=wandb_project, name=wandb_run).log_code(\".\", include_fn=lambda path: path.endswith(\".py\") or path.endswith(\".ipynb\") or path.endswith(\".json\"))\n",
    "\n",
    "step = 0\n",
    "last_step_time = time.time()\n",
    "\n",
    "for _ in range(epochs):\n",
    "    for labels, latents, prompts_encoded, prompts_atnmask in dataloader_train:\n",
    "        latents = latents * dcae_scalingf\n",
    "        latents_noisy, noise, t = add_random_noise(latents)\n",
    "        noise_pred = transformer(latents_noisy.to(dtype), prompts_encoded, t, prompts_atnmask).sample\n",
    "    \n",
    "        loss = F.mse_loss(noise_pred, noise - latents)\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()    \n",
    "        \n",
    "        if step>0 and step % steps_log == 0:\n",
    "            loss_train = loss.item()\n",
    "            step_time = (time.time() - last_step_time) / steps_log * 1000\n",
    "            sample_tp = bs * steps_log / (time.time() - last_step_time)\n",
    "            print(f\"step {step}, epoch: {step / steps_epoch:.4f}, train loss: {loss_train:.4f}, grad_norm: {grad_norm:.2f}, {step_time:.2f}ms/step, {sample_tp:.2f}samples/sec\")\n",
    "            if log_wandb: wandb.log({\"loss_train\": loss_train, \"grad_norm\": grad_norm, \"step_time\": step_time, \"step\": step, \"sample_tp\": sample_tp, \"sample_count\": step * bs, \"epoch\": step / steps_epoch})\n",
    "            last_step_time = time.time()\n",
    "    \n",
    "        if step>0 and step % steps_eval == 0:\n",
    "            transformer.eval()\n",
    "            loss_eval = eval_loss(dataloader_eval)\n",
    "            images_eval = [generate(p, tokenizer, text_encoder, latent_dim=latent_shape) for p in tqdm([eval_labels[k] for k in eval_labels], \"eval_images\")]\n",
    "            clipscore = eval_clipscore(images_eval, eval_labels)\n",
    "            print(f\"step {step}, eval loss: {loss_eval:.4f}, clipscore: {clipscore:.2f}\")\n",
    "            if log_wandb: wandb.log({\"loss_eval\": loss_eval, \"clipscore\": clipscore, \"images_eval\": wandb.Image(make_grid(images_eval, 2, 5)), \"step\": step, \"sample_count\": step * bs, \"epoch\": step / steps_epoch})\n",
    "            transformer.train()\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ec19c-1862-4c89-b46d-6206fd4f8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer.push_to_hub(f\"g-ronimo/hana-alpha14_cifar10-128_TS-{timesteps_training}_{epochs}e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2429d2e8-546c-431d-a8a7-1391011d104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !runpodctl remove pod $RUNPOD_POD_ID"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
